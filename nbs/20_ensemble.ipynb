{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp ensemble\n",
    "# | default_cls_lvl 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ensemble\n",
    "\n",
    "> TODO: Add information about this particular model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os, gc, time\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import fmin, minimize\n",
    "import torch\n",
    "\n",
    "from kaggle_comp import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide_input\n",
    "import pdb\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get OOF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('../logs')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "*_, model_output_path, log_output_path = utils.get_paths()\n",
    "log_output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../logs/oof_blurr_deberta_v3_small_2022_11_23_17_53_4_fold_4.csv',\n",
       " '../logs/oof_blurr_deberta_v3_base_2022_11_23_19_53_2_fold_2.csv',\n",
       " '../logs/oof_blurr_deberta_v3_base_2022_11_21_23_50_2_fold_2.csv',\n",
       " '../logs/oof_blurr_deberta_v3_large_2022_11_23_18_58_4_fold_4.csv',\n",
       " '../logs/oof_blurr_deberta_v3_base_2022_11_21_23_04_1_fold_1.csv']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_oof_files = glob.glob(f\"{str(log_output_path)}/oof_*.csv\")\n",
    "all_oof_files[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../logs/oof_blurr_cocolm_large_2022_11_28_14_58_0_fold_0.csv',\n",
       " '../logs/oof_blurr_cocolm_large_2022_11_28_14_58_1_fold_1.csv',\n",
       " '../logs/oof_blurr_cocolm_large_2022_11_28_14_58_2_fold_2.csv',\n",
       " '../logs/oof_blurr_cocolm_large_2022_11_28_14_58_3_fold_3.csv',\n",
       " '../logs/oof_blurr_cocolm_large_2022_11_28_14_58_4_fold_4.csv']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_models = [\n",
    "    'oof_blurr_cocolm_large_2022_11_28_14_58', \n",
    "    'oof_blurr_deberta_v3_base_2022_11_23_16_10', \n",
    "    # 'oof_blurr_deberta_v3_large_2022_11_23_16_18', \n",
    "    # 'oof_blurr_deberta_xlarge_2022_11_25_20_54',\n",
    "    # 'oof_blurr_deberta_large_2022_11_25_20_33'\n",
    "] \n",
    "\n",
    "oof_files = sorted([f for f in all_oof_files if Path(f).stem.startswith(tuple(use_models))])\n",
    "oof_files[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge OOF model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1.7001953,1.5380859,2.0488281,1.8027344,1.7763672,1.6572266])\n",
    "x[None,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.345706045627594"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func = utils.MCRMSELoss()\n",
    "loss_func(torch.tensor([1.7001953,1.5380859,2.0488281,1.8027344,1.7763672,1.6572266])[None,:], torch.tensor([2.0,2.0,2.0,2.5,2.0,2.0])[None,:]).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss_func = utils.MCRMSELoss()\n",
    "\n",
    "def calc_example_metric(r):\n",
    "    loss = loss_func(\n",
    "        torch.tensor([r[\"pred_cohesion\"],r[\"pred_syntax\"],r[\"pred_vocabulary\"],r[\"pred_phraseology\"],r[\"pred_grammar\"],r[\"pred_conventions\"]])[None,:], \n",
    "        torch.tensor([r[\"targ_cohesion\"],r[\"targ_syntax\"],r[\"targ_vocabulary\"],r[\"targ_phraseology\"],r[\"targ_grammar\"],r[\"targ_conventions\"]])[None,:]\n",
    "    )\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'oof_blurr_cocolm_large_2022_11_28_14_58'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path(oof_files[0]).stem[:-9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = None\n",
    "run_ids = set()\n",
    "for f in oof_files:\n",
    "    fold = Path(f).stem[-1]\n",
    "    \n",
    "    tmp_df = pd.read_csv(f)\n",
    "    tmp_df.insert(0, 'id', range(len(tmp_df)))\n",
    "    tmp_df.insert(1, 'fold', int(fold))\n",
    "    tmp_df[\"id\"] = tmp_df[\"id\"].apply(lambda v: f\"fold_{fold}_{str(v)}\")\n",
    "    \n",
    "    run_id = Path(f).stem[:-9] #tmp_df.iloc[0][\"run_id\"]\n",
    "    run_ids.add(run_id)\n",
    "\n",
    "    # calculate the metric for each row\n",
    "    tmp_df[f\"{run_id}_loss\"] = tmp_df.apply(calc_example_metric, axis=1)\n",
    "\n",
    "    # # move targets to end\n",
    "    # cols = list(tmp_df.columns.values)\n",
    "    # cols.pop(cols.index('Adequate')) \n",
    "    # cols.pop(cols.index('Effective')) \n",
    "    # cols.pop(cols.index('Ineffective')) \n",
    "    # tmp_df = tmp_df[cols + ['Adequate','Effective', 'Ineffective']] \n",
    "\n",
    "    # add run_id as a suffix to target label to distringuish one run from the next\n",
    "    tmp_df.rename(columns={\n",
    "        \"pred_cohesion\": f\"pred_cohesion_{run_id}\", \n",
    "        \"pred_syntax\": f\"pred_syntax_{run_id}\", \n",
    "        \"pred_vocabulary\": f\"pred_vocabulary_{run_id}\",\n",
    "        \"pred_phraseology\": f\"pred_phraseology_{run_id}\", \n",
    "        \"pred_grammar\": f\"pred_grammar_{run_id}\", \n",
    "        \"pred_conventions\": f\"pred_conventions_{run_id}\",\n",
    "    }, inplace=True)\n",
    "    \n",
    "    tmp_df.drop(columns=[\"run_id\"], inplace=True)\n",
    "\n",
    "    if df is None:\n",
    "        df = tmp_df\n",
    "    else:\n",
    "        df = df.merge(tmp_df, on=\"id\", how=\"left\", suffixes=('', '_drop'))\n",
    "        df.drop([col for col in df.columns if 'drop' in col], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>fold</th>\n",
       "      <th>pred_cohesion_oof_blurr_cocolm_large_2022_11_28_14_58</th>\n",
       "      <th>pred_syntax_oof_blurr_cocolm_large_2022_11_28_14_58</th>\n",
       "      <th>pred_vocabulary_oof_blurr_cocolm_large_2022_11_28_14_58</th>\n",
       "      <th>pred_phraseology_oof_blurr_cocolm_large_2022_11_28_14_58</th>\n",
       "      <th>pred_grammar_oof_blurr_cocolm_large_2022_11_28_14_58</th>\n",
       "      <th>pred_conventions_oof_blurr_cocolm_large_2022_11_28_14_58</th>\n",
       "      <th>targ_cohesion</th>\n",
       "      <th>targ_syntax</th>\n",
       "      <th>...</th>\n",
       "      <th>targ_conventions</th>\n",
       "      <th>grid_id</th>\n",
       "      <th>oof_blurr_cocolm_large_2022_11_28_14_58_loss</th>\n",
       "      <th>pred_cohesion_oof_blurr_deberta_v3_base_2022_11_23_16_10</th>\n",
       "      <th>pred_syntax_oof_blurr_deberta_v3_base_2022_11_23_16_10</th>\n",
       "      <th>pred_vocabulary_oof_blurr_deberta_v3_base_2022_11_23_16_10</th>\n",
       "      <th>pred_phraseology_oof_blurr_deberta_v3_base_2022_11_23_16_10</th>\n",
       "      <th>pred_grammar_oof_blurr_deberta_v3_base_2022_11_23_16_10</th>\n",
       "      <th>pred_conventions_oof_blurr_deberta_v3_base_2022_11_23_16_10</th>\n",
       "      <th>oof_blurr_deberta_v3_base_2022_11_23_16_10_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fold_0_0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.162109</td>\n",
       "      <td>3.275391</td>\n",
       "      <td>3.298828</td>\n",
       "      <td>3.296875</td>\n",
       "      <td>3.458984</td>\n",
       "      <td>3.083984</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.596030</td>\n",
       "      <td>1.724609</td>\n",
       "      <td>1.588867</td>\n",
       "      <td>2.007812</td>\n",
       "      <td>1.796875</td>\n",
       "      <td>1.785156</td>\n",
       "      <td>1.735352</td>\n",
       "      <td>0.312837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fold_0_1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.542969</td>\n",
       "      <td>3.664062</td>\n",
       "      <td>3.634766</td>\n",
       "      <td>3.691406</td>\n",
       "      <td>3.724609</td>\n",
       "      <td>3.429688</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.531251</td>\n",
       "      <td>2.925781</td>\n",
       "      <td>2.685547</td>\n",
       "      <td>3.074219</td>\n",
       "      <td>2.960938</td>\n",
       "      <td>2.923828</td>\n",
       "      <td>2.882812</td>\n",
       "      <td>0.449223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fold_0_2</td>\n",
       "      <td>0</td>\n",
       "      <td>3.638672</td>\n",
       "      <td>3.730469</td>\n",
       "      <td>3.662109</td>\n",
       "      <td>3.708984</td>\n",
       "      <td>3.638672</td>\n",
       "      <td>3.718750</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.446616</td>\n",
       "      <td>3.726562</td>\n",
       "      <td>3.644531</td>\n",
       "      <td>3.869141</td>\n",
       "      <td>3.925781</td>\n",
       "      <td>3.929688</td>\n",
       "      <td>3.835938</td>\n",
       "      <td>0.301761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fold_0_3</td>\n",
       "      <td>0</td>\n",
       "      <td>3.560547</td>\n",
       "      <td>3.529297</td>\n",
       "      <td>3.509766</td>\n",
       "      <td>3.380859</td>\n",
       "      <td>3.316406</td>\n",
       "      <td>3.742188</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.089844</td>\n",
       "      <td>3.478516</td>\n",
       "      <td>3.207031</td>\n",
       "      <td>3.427734</td>\n",
       "      <td>3.345703</td>\n",
       "      <td>2.994141</td>\n",
       "      <td>3.228516</td>\n",
       "      <td>0.385096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fold_0_4</td>\n",
       "      <td>0</td>\n",
       "      <td>3.306641</td>\n",
       "      <td>3.371094</td>\n",
       "      <td>3.410156</td>\n",
       "      <td>3.318359</td>\n",
       "      <td>3.332031</td>\n",
       "      <td>3.419922</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.417645</td>\n",
       "      <td>3.318359</td>\n",
       "      <td>3.027344</td>\n",
       "      <td>3.302734</td>\n",
       "      <td>3.158203</td>\n",
       "      <td>2.968750</td>\n",
       "      <td>3.304688</td>\n",
       "      <td>0.440434</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  fold  pred_cohesion_oof_blurr_cocolm_large_2022_11_28_14_58  \\\n",
       "0  fold_0_0     0                                               3.162109   \n",
       "1  fold_0_1     0                                               3.542969   \n",
       "2  fold_0_2     0                                               3.638672   \n",
       "3  fold_0_3     0                                               3.560547   \n",
       "4  fold_0_4     0                                               3.306641   \n",
       "\n",
       "   pred_syntax_oof_blurr_cocolm_large_2022_11_28_14_58  \\\n",
       "0                                             3.275391   \n",
       "1                                             3.664062   \n",
       "2                                             3.730469   \n",
       "3                                             3.529297   \n",
       "4                                             3.371094   \n",
       "\n",
       "   pred_vocabulary_oof_blurr_cocolm_large_2022_11_28_14_58  \\\n",
       "0                                                 3.298828   \n",
       "1                                                 3.634766   \n",
       "2                                                 3.662109   \n",
       "3                                                 3.509766   \n",
       "4                                                 3.410156   \n",
       "\n",
       "   pred_phraseology_oof_blurr_cocolm_large_2022_11_28_14_58  \\\n",
       "0                                                  3.296875   \n",
       "1                                                  3.691406   \n",
       "2                                                  3.708984   \n",
       "3                                                  3.380859   \n",
       "4                                                  3.318359   \n",
       "\n",
       "   pred_grammar_oof_blurr_cocolm_large_2022_11_28_14_58  \\\n",
       "0                                              3.458984   \n",
       "1                                              3.724609   \n",
       "2                                              3.638672   \n",
       "3                                              3.316406   \n",
       "4                                              3.332031   \n",
       "\n",
       "   pred_conventions_oof_blurr_cocolm_large_2022_11_28_14_58  targ_cohesion  \\\n",
       "0                                                  3.083984            2.5   \n",
       "1                                                  3.429688            3.0   \n",
       "2                                                  3.718750            4.5   \n",
       "3                                                  3.742188            2.0   \n",
       "4                                                  3.419922            4.0   \n",
       "\n",
       "   targ_syntax  ...  targ_conventions  grid_id  \\\n",
       "0          2.5  ...               3.0        0   \n",
       "1          3.0  ...               3.0        0   \n",
       "2          4.0  ...               4.0        0   \n",
       "3          3.0  ...               2.5        0   \n",
       "4          4.0  ...               3.5        0   \n",
       "\n",
       "   oof_blurr_cocolm_large_2022_11_28_14_58_loss  \\\n",
       "0                                      0.596030   \n",
       "1                                      0.531251   \n",
       "2                                      0.446616   \n",
       "3                                      1.089844   \n",
       "4                                      0.417645   \n",
       "\n",
       "   pred_cohesion_oof_blurr_deberta_v3_base_2022_11_23_16_10  \\\n",
       "0                                                  1.724609   \n",
       "1                                                  2.925781   \n",
       "2                                                  3.726562   \n",
       "3                                                  3.478516   \n",
       "4                                                  3.318359   \n",
       "\n",
       "   pred_syntax_oof_blurr_deberta_v3_base_2022_11_23_16_10  \\\n",
       "0                                                1.588867   \n",
       "1                                                2.685547   \n",
       "2                                                3.644531   \n",
       "3                                                3.207031   \n",
       "4                                                3.027344   \n",
       "\n",
       "   pred_vocabulary_oof_blurr_deberta_v3_base_2022_11_23_16_10  \\\n",
       "0                                                    2.007812   \n",
       "1                                                    3.074219   \n",
       "2                                                    3.869141   \n",
       "3                                                    3.427734   \n",
       "4                                                    3.302734   \n",
       "\n",
       "   pred_phraseology_oof_blurr_deberta_v3_base_2022_11_23_16_10  \\\n",
       "0                                                     1.796875   \n",
       "1                                                     2.960938   \n",
       "2                                                     3.925781   \n",
       "3                                                     3.345703   \n",
       "4                                                     3.158203   \n",
       "\n",
       "   pred_grammar_oof_blurr_deberta_v3_base_2022_11_23_16_10  \\\n",
       "0                                                 1.785156   \n",
       "1                                                 2.923828   \n",
       "2                                                 3.929688   \n",
       "3                                                 2.994141   \n",
       "4                                                 2.968750   \n",
       "\n",
       "   pred_conventions_oof_blurr_deberta_v3_base_2022_11_23_16_10  \\\n",
       "0                                                     1.735352   \n",
       "1                                                     2.882812   \n",
       "2                                                     3.835938   \n",
       "3                                                     3.228516   \n",
       "4                                                     3.304688   \n",
       "\n",
       "   oof_blurr_deberta_v3_base_2022_11_23_16_10_loss  \n",
       "0                                         0.312837  \n",
       "1                                         0.449223  \n",
       "2                                         0.301761  \n",
       "3                                         0.385096  \n",
       "4                                         0.440434  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'oof_blurr_cocolm_large_2022_11_28_14_58',\n",
       " 'oof_blurr_deberta_v3_base_2022_11_23_16_10'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. overall loss\n",
      "0.37886852147700767\n"
     ]
    }
   ],
   "source": [
    "print('Avg. overall loss')\n",
    "loss_cols = [run_id+'_loss' for run_id in run_ids]\n",
    "avg_loss = np.mean(df[loss_cols].values, axis=0)\n",
    "print(np.mean(avg_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. loss by run\n",
      "oof_blurr_cocolm_large_2022_11_28_14_58: 0.37461858342313553\n",
      "oof_blurr_deberta_v3_base_2022_11_23_16_10: 0.38311845953087975\n"
     ]
    }
   ],
   "source": [
    "print('Avg. loss by run')\n",
    "for run_id in run_ids:\n",
    "    print(f\"{run_id}: {df[run_id+'_loss'].mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize for loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prototype for how we can apply weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_weights = np.array(np.ones(len(run_ids)))\n",
    "\n",
    "for label in [\"pred_cohesion\",\"pred_syntax\",\"pred_vocabulary\",\"pred_phraseology\",\"pred_grammar\",\"pred_conventions\"]:\n",
    "    preds = []\n",
    "    for idx, run_id in enumerate(run_ids):\n",
    "        preds.append(df[f\"{label}_{run_id}\"].values * run_weights[idx])\n",
    "\n",
    "    df[label] = np.stack(preds).sum(axis=0) / run_weights.sum()\n",
    "\n",
    "df[f\"weighted_loss\"] = df.apply(calc_example_metric, axis=1)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.456843372710678"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[f\"weighted_loss\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>fold</th>\n",
       "      <th>pred_cohesion_oof_blurr_cocolm_large_2022_11_28_14_58</th>\n",
       "      <th>pred_syntax_oof_blurr_cocolm_large_2022_11_28_14_58</th>\n",
       "      <th>pred_vocabulary_oof_blurr_cocolm_large_2022_11_28_14_58</th>\n",
       "      <th>pred_phraseology_oof_blurr_cocolm_large_2022_11_28_14_58</th>\n",
       "      <th>pred_grammar_oof_blurr_cocolm_large_2022_11_28_14_58</th>\n",
       "      <th>pred_conventions_oof_blurr_cocolm_large_2022_11_28_14_58</th>\n",
       "      <th>targ_cohesion</th>\n",
       "      <th>targ_syntax</th>\n",
       "      <th>...</th>\n",
       "      <th>pred_grammar_oof_blurr_deberta_v3_base_2022_11_23_16_10</th>\n",
       "      <th>pred_conventions_oof_blurr_deberta_v3_base_2022_11_23_16_10</th>\n",
       "      <th>oof_blurr_deberta_v3_base_2022_11_23_16_10_loss</th>\n",
       "      <th>pred_cohesion</th>\n",
       "      <th>pred_syntax</th>\n",
       "      <th>pred_vocabulary</th>\n",
       "      <th>pred_phraseology</th>\n",
       "      <th>pred_grammar</th>\n",
       "      <th>pred_conventions</th>\n",
       "      <th>weighted_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fold_0_0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.162109</td>\n",
       "      <td>3.275391</td>\n",
       "      <td>3.298828</td>\n",
       "      <td>3.296875</td>\n",
       "      <td>3.458984</td>\n",
       "      <td>3.083984</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>...</td>\n",
       "      <td>1.785156</td>\n",
       "      <td>1.735352</td>\n",
       "      <td>0.312837</td>\n",
       "      <td>2.443359</td>\n",
       "      <td>2.432129</td>\n",
       "      <td>2.653320</td>\n",
       "      <td>2.546875</td>\n",
       "      <td>2.622070</td>\n",
       "      <td>2.409668</td>\n",
       "      <td>0.205084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fold_0_1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.542969</td>\n",
       "      <td>3.664062</td>\n",
       "      <td>3.634766</td>\n",
       "      <td>3.691406</td>\n",
       "      <td>3.724609</td>\n",
       "      <td>3.429688</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.923828</td>\n",
       "      <td>2.882812</td>\n",
       "      <td>0.449223</td>\n",
       "      <td>3.234375</td>\n",
       "      <td>3.174805</td>\n",
       "      <td>3.354492</td>\n",
       "      <td>3.326172</td>\n",
       "      <td>3.324219</td>\n",
       "      <td>3.156250</td>\n",
       "      <td>0.236982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fold_0_2</td>\n",
       "      <td>0</td>\n",
       "      <td>3.638672</td>\n",
       "      <td>3.730469</td>\n",
       "      <td>3.662109</td>\n",
       "      <td>3.708984</td>\n",
       "      <td>3.638672</td>\n",
       "      <td>3.718750</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.929688</td>\n",
       "      <td>3.835938</td>\n",
       "      <td>0.301761</td>\n",
       "      <td>3.682617</td>\n",
       "      <td>3.687500</td>\n",
       "      <td>3.765625</td>\n",
       "      <td>3.817383</td>\n",
       "      <td>3.784180</td>\n",
       "      <td>3.777344</td>\n",
       "      <td>0.425620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fold_0_3</td>\n",
       "      <td>0</td>\n",
       "      <td>3.560547</td>\n",
       "      <td>3.529297</td>\n",
       "      <td>3.509766</td>\n",
       "      <td>3.380859</td>\n",
       "      <td>3.316406</td>\n",
       "      <td>3.742188</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.994141</td>\n",
       "      <td>3.228516</td>\n",
       "      <td>0.385096</td>\n",
       "      <td>3.519531</td>\n",
       "      <td>3.368164</td>\n",
       "      <td>3.468750</td>\n",
       "      <td>3.363281</td>\n",
       "      <td>3.155273</td>\n",
       "      <td>3.485352</td>\n",
       "      <td>0.976726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fold_0_4</td>\n",
       "      <td>0</td>\n",
       "      <td>3.306641</td>\n",
       "      <td>3.371094</td>\n",
       "      <td>3.410156</td>\n",
       "      <td>3.318359</td>\n",
       "      <td>3.332031</td>\n",
       "      <td>3.419922</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.968750</td>\n",
       "      <td>3.304688</td>\n",
       "      <td>0.440434</td>\n",
       "      <td>3.312500</td>\n",
       "      <td>3.199219</td>\n",
       "      <td>3.356445</td>\n",
       "      <td>3.238281</td>\n",
       "      <td>3.150391</td>\n",
       "      <td>3.362305</td>\n",
       "      <td>0.446942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>fold_0_5</td>\n",
       "      <td>0</td>\n",
       "      <td>2.839844</td>\n",
       "      <td>2.775391</td>\n",
       "      <td>2.990234</td>\n",
       "      <td>2.757812</td>\n",
       "      <td>2.839844</td>\n",
       "      <td>2.908203</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>...</td>\n",
       "      <td>2.958984</td>\n",
       "      <td>3.390625</td>\n",
       "      <td>0.331384</td>\n",
       "      <td>3.171875</td>\n",
       "      <td>2.977539</td>\n",
       "      <td>3.215820</td>\n",
       "      <td>3.018555</td>\n",
       "      <td>2.899414</td>\n",
       "      <td>3.149414</td>\n",
       "      <td>0.272301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>fold_0_6</td>\n",
       "      <td>0</td>\n",
       "      <td>2.673828</td>\n",
       "      <td>2.494141</td>\n",
       "      <td>2.884766</td>\n",
       "      <td>2.597656</td>\n",
       "      <td>2.544922</td>\n",
       "      <td>2.593750</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>...</td>\n",
       "      <td>2.808594</td>\n",
       "      <td>3.339844</td>\n",
       "      <td>0.260094</td>\n",
       "      <td>3.098633</td>\n",
       "      <td>2.841797</td>\n",
       "      <td>3.152344</td>\n",
       "      <td>2.916992</td>\n",
       "      <td>2.676758</td>\n",
       "      <td>2.966797</td>\n",
       "      <td>0.469891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>fold_0_7</td>\n",
       "      <td>0</td>\n",
       "      <td>2.775391</td>\n",
       "      <td>2.666016</td>\n",
       "      <td>2.943359</td>\n",
       "      <td>2.742188</td>\n",
       "      <td>2.771484</td>\n",
       "      <td>2.650391</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>...</td>\n",
       "      <td>3.529297</td>\n",
       "      <td>3.490234</td>\n",
       "      <td>0.408859</td>\n",
       "      <td>3.134766</td>\n",
       "      <td>3.004883</td>\n",
       "      <td>3.239258</td>\n",
       "      <td>3.151367</td>\n",
       "      <td>3.150391</td>\n",
       "      <td>3.070312</td>\n",
       "      <td>0.462079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>fold_0_8</td>\n",
       "      <td>0</td>\n",
       "      <td>2.880859</td>\n",
       "      <td>2.738281</td>\n",
       "      <td>2.976562</td>\n",
       "      <td>2.794922</td>\n",
       "      <td>2.783203</td>\n",
       "      <td>2.625000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.916016</td>\n",
       "      <td>2.697266</td>\n",
       "      <td>0.470055</td>\n",
       "      <td>2.870117</td>\n",
       "      <td>2.702148</td>\n",
       "      <td>3.012695</td>\n",
       "      <td>2.889648</td>\n",
       "      <td>2.849609</td>\n",
       "      <td>2.661133</td>\n",
       "      <td>0.340015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fold_0_9</td>\n",
       "      <td>0</td>\n",
       "      <td>2.966797</td>\n",
       "      <td>2.871094</td>\n",
       "      <td>3.125000</td>\n",
       "      <td>2.890625</td>\n",
       "      <td>2.783203</td>\n",
       "      <td>2.953125</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>...</td>\n",
       "      <td>2.777344</td>\n",
       "      <td>2.630859</td>\n",
       "      <td>0.299488</td>\n",
       "      <td>2.895508</td>\n",
       "      <td>2.735352</td>\n",
       "      <td>3.069336</td>\n",
       "      <td>2.895508</td>\n",
       "      <td>2.780273</td>\n",
       "      <td>2.791992</td>\n",
       "      <td>0.661786</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  fold  pred_cohesion_oof_blurr_cocolm_large_2022_11_28_14_58  \\\n",
       "0  fold_0_0     0                                               3.162109   \n",
       "1  fold_0_1     0                                               3.542969   \n",
       "2  fold_0_2     0                                               3.638672   \n",
       "3  fold_0_3     0                                               3.560547   \n",
       "4  fold_0_4     0                                               3.306641   \n",
       "5  fold_0_5     0                                               2.839844   \n",
       "6  fold_0_6     0                                               2.673828   \n",
       "7  fold_0_7     0                                               2.775391   \n",
       "8  fold_0_8     0                                               2.880859   \n",
       "9  fold_0_9     0                                               2.966797   \n",
       "\n",
       "   pred_syntax_oof_blurr_cocolm_large_2022_11_28_14_58  \\\n",
       "0                                             3.275391   \n",
       "1                                             3.664062   \n",
       "2                                             3.730469   \n",
       "3                                             3.529297   \n",
       "4                                             3.371094   \n",
       "5                                             2.775391   \n",
       "6                                             2.494141   \n",
       "7                                             2.666016   \n",
       "8                                             2.738281   \n",
       "9                                             2.871094   \n",
       "\n",
       "   pred_vocabulary_oof_blurr_cocolm_large_2022_11_28_14_58  \\\n",
       "0                                                 3.298828   \n",
       "1                                                 3.634766   \n",
       "2                                                 3.662109   \n",
       "3                                                 3.509766   \n",
       "4                                                 3.410156   \n",
       "5                                                 2.990234   \n",
       "6                                                 2.884766   \n",
       "7                                                 2.943359   \n",
       "8                                                 2.976562   \n",
       "9                                                 3.125000   \n",
       "\n",
       "   pred_phraseology_oof_blurr_cocolm_large_2022_11_28_14_58  \\\n",
       "0                                                  3.296875   \n",
       "1                                                  3.691406   \n",
       "2                                                  3.708984   \n",
       "3                                                  3.380859   \n",
       "4                                                  3.318359   \n",
       "5                                                  2.757812   \n",
       "6                                                  2.597656   \n",
       "7                                                  2.742188   \n",
       "8                                                  2.794922   \n",
       "9                                                  2.890625   \n",
       "\n",
       "   pred_grammar_oof_blurr_cocolm_large_2022_11_28_14_58  \\\n",
       "0                                              3.458984   \n",
       "1                                              3.724609   \n",
       "2                                              3.638672   \n",
       "3                                              3.316406   \n",
       "4                                              3.332031   \n",
       "5                                              2.839844   \n",
       "6                                              2.544922   \n",
       "7                                              2.771484   \n",
       "8                                              2.783203   \n",
       "9                                              2.783203   \n",
       "\n",
       "   pred_conventions_oof_blurr_cocolm_large_2022_11_28_14_58  targ_cohesion  \\\n",
       "0                                                  3.083984            2.5   \n",
       "1                                                  3.429688            3.0   \n",
       "2                                                  3.718750            4.5   \n",
       "3                                                  3.742188            2.0   \n",
       "4                                                  3.419922            4.0   \n",
       "5                                                  2.908203            3.0   \n",
       "6                                                  2.593750            2.5   \n",
       "7                                                  2.650391            2.0   \n",
       "8                                                  2.625000            4.0   \n",
       "9                                                  2.953125            3.5   \n",
       "\n",
       "   targ_syntax  ...  pred_grammar_oof_blurr_deberta_v3_base_2022_11_23_16_10  \\\n",
       "0          2.5  ...                                                 1.785156   \n",
       "1          3.0  ...                                                 2.923828   \n",
       "2          4.0  ...                                                 3.929688   \n",
       "3          3.0  ...                                                 2.994141   \n",
       "4          4.0  ...                                                 2.968750   \n",
       "5          2.5  ...                                                 2.958984   \n",
       "6          2.5  ...                                                 2.808594   \n",
       "7          2.5  ...                                                 3.529297   \n",
       "8          3.0  ...                                                 2.916016   \n",
       "9          3.5  ...                                                 2.777344   \n",
       "\n",
       "   pred_conventions_oof_blurr_deberta_v3_base_2022_11_23_16_10  \\\n",
       "0                                                     1.735352   \n",
       "1                                                     2.882812   \n",
       "2                                                     3.835938   \n",
       "3                                                     3.228516   \n",
       "4                                                     3.304688   \n",
       "5                                                     3.390625   \n",
       "6                                                     3.339844   \n",
       "7                                                     3.490234   \n",
       "8                                                     2.697266   \n",
       "9                                                     2.630859   \n",
       "\n",
       "   oof_blurr_deberta_v3_base_2022_11_23_16_10_loss  pred_cohesion  \\\n",
       "0                                         0.312837       2.443359   \n",
       "1                                         0.449223       3.234375   \n",
       "2                                         0.301761       3.682617   \n",
       "3                                         0.385096       3.519531   \n",
       "4                                         0.440434       3.312500   \n",
       "5                                         0.331384       3.171875   \n",
       "6                                         0.260094       3.098633   \n",
       "7                                         0.408859       3.134766   \n",
       "8                                         0.470055       2.870117   \n",
       "9                                         0.299488       2.895508   \n",
       "\n",
       "   pred_syntax  pred_vocabulary  pred_phraseology  pred_grammar  \\\n",
       "0     2.432129         2.653320          2.546875      2.622070   \n",
       "1     3.174805         3.354492          3.326172      3.324219   \n",
       "2     3.687500         3.765625          3.817383      3.784180   \n",
       "3     3.368164         3.468750          3.363281      3.155273   \n",
       "4     3.199219         3.356445          3.238281      3.150391   \n",
       "5     2.977539         3.215820          3.018555      2.899414   \n",
       "6     2.841797         3.152344          2.916992      2.676758   \n",
       "7     3.004883         3.239258          3.151367      3.150391   \n",
       "8     2.702148         3.012695          2.889648      2.849609   \n",
       "9     2.735352         3.069336          2.895508      2.780273   \n",
       "\n",
       "   pred_conventions  weighted_loss  \n",
       "0          2.409668       0.205084  \n",
       "1          3.156250       0.236982  \n",
       "2          3.777344       0.425620  \n",
       "3          3.485352       0.976726  \n",
       "4          3.362305       0.446942  \n",
       "5          3.149414       0.272301  \n",
       "6          2.966797       0.469891  \n",
       "7          3.070312       0.462079  \n",
       "8          2.661133       0.340015  \n",
       "9          2.791992       0.661786  \n",
       "\n",
       "[10 rows x 30 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizeMetric:\n",
    "    def __init__(self, run_ids) -> None:\n",
    "        self.coefs_ = 0\n",
    "        self.run_ids = run_ids\n",
    "        self.n_models = len(self.run_ids)\n",
    "\n",
    "    def _optimize(self, coefs, train_df, valid_df):\n",
    "        for label in [\"pred_cohesion\",\"pred_syntax\",\"pred_vocabulary\",\"pred_phraseology\",\"pred_grammar\",\"pred_conventions\"]:\n",
    "            preds = []\n",
    "            for idx, run_id in enumerate(self.run_ids):\n",
    "                preds.append(valid_df[f\"{label}_{run_id}\"].values * coefs[idx])\n",
    "\n",
    "            valid_df[label] = np.stack(preds).sum(axis=0) / coefs.sum()\n",
    "        res = valid_df.apply(calc_example_metric, axis=1).mean()\n",
    "        # print(res)\n",
    "        return res\n",
    "\n",
    "    def fit(self, train_df, valid_df):\n",
    "        optimize_func = partial(self._optimize, train_df=train_df.copy(), valid_df=valid_df.copy())\n",
    "        init_coefs = np.random.dirichlet(np.ones(self.n_models))\n",
    "        optimize_res = fmin(optimize_func, init_coefs, disp=True)\n",
    "        # optimize_res = minimize(optimize_func, init_coefs, method='Nelder-Mead', options={'maxiter': 10000, 'maxfev': 8000})\n",
    "        self.coefs_ = optimize_res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_model_coefs(res_df, fold):\n",
    "    train_df = res_df[res_df[\"fold\"] != fold].reset_index(drop=True)\n",
    "    valid_df = res_df[res_df[\"fold\"] == fold].reset_index(drop=True)\n",
    "\n",
    "    opt_metric = OptimizeMetric(run_ids)\n",
    "    opt_metric.fit(train_df, valid_df)\n",
    "    return opt_metric.coefs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.374337\n",
      "         Iterations: 37\n",
      "         Function evaluations: 82\n",
      "Fold 0: [0.61446169 0.0150783 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1394859/3230640729.py:21: RuntimeWarning: Maximum number of function evaluations has been exceeded.\n",
      "  optimize_res = fmin(optimize_func, init_coefs, disp=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: [0.17744436 0.82255564]\n",
      "Fold 2: [0.25141205 0.74858795]\n",
      "Fold 3: [0.80100426 0.19899574]\n",
      "Fold 4: [0.28699177 0.71300823]\n"
     ]
    }
   ],
   "source": [
    "fold_coefs = []\n",
    "for f_idx in range(5):\n",
    "    coefs = tune_model_coefs(df, f_idx)\n",
    "    fold_coefs.append(coefs)\n",
    "\n",
    "    print(f\"Fold {f_idx}: {coefs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.61446169, 0.0150783 ]),\n",
       " array([0.17744436, 0.82255564]),\n",
       " array([0.25141205, 0.74858795]),\n",
       " array([0.80100426, 0.19899574]),\n",
       " array([0.28699177, 0.71300823])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold_coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.61446169, 0.17744436, 0.25141205, 0.80100426, 0.28699177],\n",
       "       [0.0150783 , 0.82255564, 0.74858795, 0.19899574, 0.71300823]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(fold_coefs).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'oof_blurr_cocolm_large_2022_11_28_14_58',\n",
       " 'oof_blurr_deberta_v3_base_2022_11_23_16_10'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df = pd.DataFrame()\n",
    "\n",
    "for f_idx, coefs in enumerate(fold_coefs):\n",
    "    oof_df = df[df[\"fold\"] == f_idx].reset_index(drop=True)\n",
    "    \n",
    "    for label in [\"pred_cohesion\",\"pred_syntax\",\"pred_vocabulary\",\"pred_phraseology\",\"pred_grammar\",\"pred_conventions\"]:\n",
    "        preds = []\n",
    "        for idx, run_id in enumerate(run_ids):\n",
    "            preds.append(oof_df[f\"{label}_{run_id}\"].values * coefs[idx])\n",
    "\n",
    "        oof_df[label] = np.stack(preds).sum(axis=0) / coefs.sum()\n",
    "\n",
    "    oof_df[f\"weighted_loss\"] = df.apply(calc_example_metric, axis=1)\n",
    "    preds_df = pd.concat([preds_df, oof_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3910\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>fold</th>\n",
       "      <th>pred_cohesion_oof_blurr_cocolm_large_2022_11_28_14_58</th>\n",
       "      <th>pred_syntax_oof_blurr_cocolm_large_2022_11_28_14_58</th>\n",
       "      <th>pred_vocabulary_oof_blurr_cocolm_large_2022_11_28_14_58</th>\n",
       "      <th>pred_phraseology_oof_blurr_cocolm_large_2022_11_28_14_58</th>\n",
       "      <th>pred_grammar_oof_blurr_cocolm_large_2022_11_28_14_58</th>\n",
       "      <th>pred_conventions_oof_blurr_cocolm_large_2022_11_28_14_58</th>\n",
       "      <th>targ_cohesion</th>\n",
       "      <th>targ_syntax</th>\n",
       "      <th>...</th>\n",
       "      <th>pred_grammar_oof_blurr_deberta_v3_base_2022_11_23_16_10</th>\n",
       "      <th>pred_conventions_oof_blurr_deberta_v3_base_2022_11_23_16_10</th>\n",
       "      <th>oof_blurr_deberta_v3_base_2022_11_23_16_10_loss</th>\n",
       "      <th>pred_cohesion</th>\n",
       "      <th>pred_syntax</th>\n",
       "      <th>pred_vocabulary</th>\n",
       "      <th>pred_phraseology</th>\n",
       "      <th>pred_grammar</th>\n",
       "      <th>pred_conventions</th>\n",
       "      <th>weighted_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fold_0_0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.162109</td>\n",
       "      <td>3.275391</td>\n",
       "      <td>3.298828</td>\n",
       "      <td>3.296875</td>\n",
       "      <td>3.458984</td>\n",
       "      <td>3.083984</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>...</td>\n",
       "      <td>1.785156</td>\n",
       "      <td>1.735352</td>\n",
       "      <td>0.312837</td>\n",
       "      <td>3.127679</td>\n",
       "      <td>3.234996</td>\n",
       "      <td>3.267907</td>\n",
       "      <td>3.260948</td>\n",
       "      <td>3.418894</td>\n",
       "      <td>3.051683</td>\n",
       "      <td>0.205084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fold_0_1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.542969</td>\n",
       "      <td>3.664062</td>\n",
       "      <td>3.634766</td>\n",
       "      <td>3.691406</td>\n",
       "      <td>3.724609</td>\n",
       "      <td>3.429688</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.923828</td>\n",
       "      <td>2.882812</td>\n",
       "      <td>0.449223</td>\n",
       "      <td>3.528186</td>\n",
       "      <td>3.640626</td>\n",
       "      <td>3.621340</td>\n",
       "      <td>3.673911</td>\n",
       "      <td>3.705430</td>\n",
       "      <td>3.416589</td>\n",
       "      <td>0.236982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fold_0_2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.638672</td>\n",
       "      <td>3.730469</td>\n",
       "      <td>3.662109</td>\n",
       "      <td>3.708984</td>\n",
       "      <td>3.638672</td>\n",
       "      <td>3.718750</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.929688</td>\n",
       "      <td>3.835938</td>\n",
       "      <td>0.301761</td>\n",
       "      <td>3.640777</td>\n",
       "      <td>3.728410</td>\n",
       "      <td>3.667068</td>\n",
       "      <td>3.714177</td>\n",
       "      <td>3.645642</td>\n",
       "      <td>3.721557</td>\n",
       "      <td>0.425620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fold_0_3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.560547</td>\n",
       "      <td>3.529297</td>\n",
       "      <td>3.509766</td>\n",
       "      <td>3.380859</td>\n",
       "      <td>3.316406</td>\n",
       "      <td>3.742188</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.994141</td>\n",
       "      <td>3.228516</td>\n",
       "      <td>0.385096</td>\n",
       "      <td>3.558582</td>\n",
       "      <td>3.521578</td>\n",
       "      <td>3.507801</td>\n",
       "      <td>3.380017</td>\n",
       "      <td>3.308688</td>\n",
       "      <td>3.729884</td>\n",
       "      <td>0.976726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fold_0_4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.306641</td>\n",
       "      <td>3.371094</td>\n",
       "      <td>3.410156</td>\n",
       "      <td>3.318359</td>\n",
       "      <td>3.332031</td>\n",
       "      <td>3.419922</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.968750</td>\n",
       "      <td>3.304688</td>\n",
       "      <td>0.440434</td>\n",
       "      <td>3.306921</td>\n",
       "      <td>3.362861</td>\n",
       "      <td>3.407583</td>\n",
       "      <td>3.314523</td>\n",
       "      <td>3.323330</td>\n",
       "      <td>3.417162</td>\n",
       "      <td>0.446942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>fold_0_5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.839844</td>\n",
       "      <td>2.775391</td>\n",
       "      <td>2.990234</td>\n",
       "      <td>2.757812</td>\n",
       "      <td>2.839844</td>\n",
       "      <td>2.908203</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>...</td>\n",
       "      <td>2.958984</td>\n",
       "      <td>3.390625</td>\n",
       "      <td>0.331384</td>\n",
       "      <td>2.855749</td>\n",
       "      <td>2.785074</td>\n",
       "      <td>3.001041</td>\n",
       "      <td>2.770303</td>\n",
       "      <td>2.842697</td>\n",
       "      <td>2.919758</td>\n",
       "      <td>0.272301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>fold_0_6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.673828</td>\n",
       "      <td>2.494141</td>\n",
       "      <td>2.884766</td>\n",
       "      <td>2.597656</td>\n",
       "      <td>2.544922</td>\n",
       "      <td>2.593750</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>...</td>\n",
       "      <td>2.808594</td>\n",
       "      <td>3.339844</td>\n",
       "      <td>0.260094</td>\n",
       "      <td>2.694177</td>\n",
       "      <td>2.510794</td>\n",
       "      <td>2.897583</td>\n",
       "      <td>2.612953</td>\n",
       "      <td>2.551237</td>\n",
       "      <td>2.611620</td>\n",
       "      <td>0.469891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>fold_0_7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.775391</td>\n",
       "      <td>2.666016</td>\n",
       "      <td>2.943359</td>\n",
       "      <td>2.742188</td>\n",
       "      <td>2.771484</td>\n",
       "      <td>2.650391</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>...</td>\n",
       "      <td>3.529297</td>\n",
       "      <td>3.490234</td>\n",
       "      <td>0.408859</td>\n",
       "      <td>2.792606</td>\n",
       "      <td>2.682248</td>\n",
       "      <td>2.957534</td>\n",
       "      <td>2.761788</td>\n",
       "      <td>2.789635</td>\n",
       "      <td>2.670506</td>\n",
       "      <td>0.462079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>fold_0_8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.880859</td>\n",
       "      <td>2.738281</td>\n",
       "      <td>2.976562</td>\n",
       "      <td>2.794922</td>\n",
       "      <td>2.783203</td>\n",
       "      <td>2.625000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.916016</td>\n",
       "      <td>2.697266</td>\n",
       "      <td>0.470055</td>\n",
       "      <td>2.880345</td>\n",
       "      <td>2.736550</td>\n",
       "      <td>2.978293</td>\n",
       "      <td>2.799460</td>\n",
       "      <td>2.786384</td>\n",
       "      <td>2.626731</td>\n",
       "      <td>0.340015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fold_0_9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.966797</td>\n",
       "      <td>2.871094</td>\n",
       "      <td>3.125000</td>\n",
       "      <td>2.890625</td>\n",
       "      <td>2.783203</td>\n",
       "      <td>2.953125</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>...</td>\n",
       "      <td>2.777344</td>\n",
       "      <td>2.630859</td>\n",
       "      <td>0.299488</td>\n",
       "      <td>2.963382</td>\n",
       "      <td>2.864591</td>\n",
       "      <td>3.122334</td>\n",
       "      <td>2.890859</td>\n",
       "      <td>2.783063</td>\n",
       "      <td>2.945406</td>\n",
       "      <td>0.661786</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  fold  pred_cohesion_oof_blurr_cocolm_large_2022_11_28_14_58  \\\n",
       "0  fold_0_0   0.0                                               3.162109   \n",
       "1  fold_0_1   0.0                                               3.542969   \n",
       "2  fold_0_2   0.0                                               3.638672   \n",
       "3  fold_0_3   0.0                                               3.560547   \n",
       "4  fold_0_4   0.0                                               3.306641   \n",
       "5  fold_0_5   0.0                                               2.839844   \n",
       "6  fold_0_6   0.0                                               2.673828   \n",
       "7  fold_0_7   0.0                                               2.775391   \n",
       "8  fold_0_8   0.0                                               2.880859   \n",
       "9  fold_0_9   0.0                                               2.966797   \n",
       "\n",
       "   pred_syntax_oof_blurr_cocolm_large_2022_11_28_14_58  \\\n",
       "0                                             3.275391   \n",
       "1                                             3.664062   \n",
       "2                                             3.730469   \n",
       "3                                             3.529297   \n",
       "4                                             3.371094   \n",
       "5                                             2.775391   \n",
       "6                                             2.494141   \n",
       "7                                             2.666016   \n",
       "8                                             2.738281   \n",
       "9                                             2.871094   \n",
       "\n",
       "   pred_vocabulary_oof_blurr_cocolm_large_2022_11_28_14_58  \\\n",
       "0                                                 3.298828   \n",
       "1                                                 3.634766   \n",
       "2                                                 3.662109   \n",
       "3                                                 3.509766   \n",
       "4                                                 3.410156   \n",
       "5                                                 2.990234   \n",
       "6                                                 2.884766   \n",
       "7                                                 2.943359   \n",
       "8                                                 2.976562   \n",
       "9                                                 3.125000   \n",
       "\n",
       "   pred_phraseology_oof_blurr_cocolm_large_2022_11_28_14_58  \\\n",
       "0                                                  3.296875   \n",
       "1                                                  3.691406   \n",
       "2                                                  3.708984   \n",
       "3                                                  3.380859   \n",
       "4                                                  3.318359   \n",
       "5                                                  2.757812   \n",
       "6                                                  2.597656   \n",
       "7                                                  2.742188   \n",
       "8                                                  2.794922   \n",
       "9                                                  2.890625   \n",
       "\n",
       "   pred_grammar_oof_blurr_cocolm_large_2022_11_28_14_58  \\\n",
       "0                                              3.458984   \n",
       "1                                              3.724609   \n",
       "2                                              3.638672   \n",
       "3                                              3.316406   \n",
       "4                                              3.332031   \n",
       "5                                              2.839844   \n",
       "6                                              2.544922   \n",
       "7                                              2.771484   \n",
       "8                                              2.783203   \n",
       "9                                              2.783203   \n",
       "\n",
       "   pred_conventions_oof_blurr_cocolm_large_2022_11_28_14_58  targ_cohesion  \\\n",
       "0                                                  3.083984            2.5   \n",
       "1                                                  3.429688            3.0   \n",
       "2                                                  3.718750            4.5   \n",
       "3                                                  3.742188            2.0   \n",
       "4                                                  3.419922            4.0   \n",
       "5                                                  2.908203            3.0   \n",
       "6                                                  2.593750            2.5   \n",
       "7                                                  2.650391            2.0   \n",
       "8                                                  2.625000            4.0   \n",
       "9                                                  2.953125            3.5   \n",
       "\n",
       "   targ_syntax  ...  pred_grammar_oof_blurr_deberta_v3_base_2022_11_23_16_10  \\\n",
       "0          2.5  ...                                                 1.785156   \n",
       "1          3.0  ...                                                 2.923828   \n",
       "2          4.0  ...                                                 3.929688   \n",
       "3          3.0  ...                                                 2.994141   \n",
       "4          4.0  ...                                                 2.968750   \n",
       "5          2.5  ...                                                 2.958984   \n",
       "6          2.5  ...                                                 2.808594   \n",
       "7          2.5  ...                                                 3.529297   \n",
       "8          3.0  ...                                                 2.916016   \n",
       "9          3.5  ...                                                 2.777344   \n",
       "\n",
       "   pred_conventions_oof_blurr_deberta_v3_base_2022_11_23_16_10  \\\n",
       "0                                                     1.735352   \n",
       "1                                                     2.882812   \n",
       "2                                                     3.835938   \n",
       "3                                                     3.228516   \n",
       "4                                                     3.304688   \n",
       "5                                                     3.390625   \n",
       "6                                                     3.339844   \n",
       "7                                                     3.490234   \n",
       "8                                                     2.697266   \n",
       "9                                                     2.630859   \n",
       "\n",
       "   oof_blurr_deberta_v3_base_2022_11_23_16_10_loss  pred_cohesion  \\\n",
       "0                                         0.312837       3.127679   \n",
       "1                                         0.449223       3.528186   \n",
       "2                                         0.301761       3.640777   \n",
       "3                                         0.385096       3.558582   \n",
       "4                                         0.440434       3.306921   \n",
       "5                                         0.331384       2.855749   \n",
       "6                                         0.260094       2.694177   \n",
       "7                                         0.408859       2.792606   \n",
       "8                                         0.470055       2.880345   \n",
       "9                                         0.299488       2.963382   \n",
       "\n",
       "   pred_syntax  pred_vocabulary  pred_phraseology  pred_grammar  \\\n",
       "0     3.234996         3.267907          3.260948      3.418894   \n",
       "1     3.640626         3.621340          3.673911      3.705430   \n",
       "2     3.728410         3.667068          3.714177      3.645642   \n",
       "3     3.521578         3.507801          3.380017      3.308688   \n",
       "4     3.362861         3.407583          3.314523      3.323330   \n",
       "5     2.785074         3.001041          2.770303      2.842697   \n",
       "6     2.510794         2.897583          2.612953      2.551237   \n",
       "7     2.682248         2.957534          2.761788      2.789635   \n",
       "8     2.736550         2.978293          2.799460      2.786384   \n",
       "9     2.864591         3.122334          2.890859      2.783063   \n",
       "\n",
       "   pred_conventions  weighted_loss  \n",
       "0          3.051683       0.205084  \n",
       "1          3.416589       0.236982  \n",
       "2          3.721557       0.425620  \n",
       "3          3.729884       0.976726  \n",
       "4          3.417162       0.446942  \n",
       "5          2.919758       0.272301  \n",
       "6          2.611620       0.469891  \n",
       "7          2.670506       0.462079  \n",
       "8          2.626731       0.340015  \n",
       "9          2.945406       0.661786  \n",
       "\n",
       "[10 rows x 30 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(preds_df))\n",
    "preds_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. weighted loss\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.456843372710678"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Avg. weighted loss\")\n",
    "preds_df.weighted_loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('kaggle_feedback_ell')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
