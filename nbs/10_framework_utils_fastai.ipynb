{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp framework_utils.fastai\n",
    "#| default_cls_lvl 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# framework_utils.fastai\n",
    "\n",
    "> As Jeremy says in his notebook, \"Iteration speed is critical, so we need to quickly be able to try different data processing and trainer parameters\" ... so lets create some helper methods to help us iterate more quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os, gc, time\n",
    "\n",
    "from fastai.callback.all import *\n",
    "from fastai.data.block import ColReader, ColSplitter, DataBlock, IndexSplitter, RegressionBlock\n",
    "from fastai.imports import *\n",
    "from fastai.layers import SigmoidRange\n",
    "from fastai.learner import *\n",
    "from fastai.losses import MSELossFlat, L1LossFlat\n",
    "from fastai.optimizer import Adam\n",
    "from fastai.metrics import *\n",
    "from fastai.torch_core import *\n",
    "from fastai.torch_imports import *\n",
    "from transformers import AutoConfig, AutoModelForMaskedLM, AutoModelForSequenceClassification, logging\n",
    "\n",
    "from torch.nn import SmoothL1Loss\n",
    "\n",
    "from blurr.callbacks import GradientCheckpointing\n",
    "from blurr.text.data.core import TextBlock\n",
    "from blurr.text.modeling.core import BaseModelWrapper, BaseModelCallback, blurr_splitter\n",
    "from blurr.text.utils import get_hf_objects\n",
    "from blurr.utils import PreCalculatedMSELoss, set_seed\n",
    "\n",
    "from kaggle_comp import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide_input\n",
    "import pdb\n",
    "from fastcore.test import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU #0: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "#| cuda\n",
    "torch.cuda.set_device(0)\n",
    "print(f\"Using GPU #{torch.cuda.current_device()}: {torch.cuda.get_device_name()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face `transformers` objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_comp_hf_objects(\n",
    "    model_checkpoint,\n",
    "    model_cls=AutoModelForSequenceClassification,\n",
    "    config_kwargs={\"num_labels\": 6},\n",
    "    tokenizer_kwargs={},\n",
    "    model_kwargs={},\n",
    "    new_special_tokens=None,\n",
    "):\n",
    "\n",
    "    # need to create configuration object separately because we may be adding new attributes (e.g., cls_dropout)\n",
    "    config = AutoConfig.from_pretrained(model_checkpoint)\n",
    "    config.update(config_kwargs)\n",
    "\n",
    "    hf_arch, hf_config, hf_tokenizer, hf_model = get_hf_objects(\n",
    "        model_checkpoint,\n",
    "        model_cls=model_cls,\n",
    "        config=config,\n",
    "        tokenizer_kwargs=tokenizer_kwargs,\n",
    "        model_kwargs=model_kwargs,\n",
    "    )\n",
    "\n",
    "    if new_special_tokens:\n",
    "        # After adding the new tokens, we need to resize the embedding matrix in the model and initialize the weights\n",
    "        hf_tokenizer.add_special_tokens({\"additional_special_tokens\": new_special_tokens})\n",
    "        hf_model.resize_token_embeddings(len(hf_tokenizer))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            emb_size = hf_model.config.to_dict().get(\"embedding_size\", hf_model.config.hidden_size)\n",
    "            hf_model.get_input_embeddings().weight[-len(hf_tokenizer), :] = torch.zeros([emb_size])\n",
    "\n",
    "    return hf_arch, hf_config, hf_tokenizer, hf_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `DataLoaders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# NOTE: This is needed for the older models (since renamed to just \"build_inputs\")\n",
    "def build_inputs_with_sep_and_section_toks(example, sep=\" - \", lower_case=False, col=\"section_tok\"):\n",
    "    if lower_case:\n",
    "        return f'{example[col].lower()}{sep}{example[\"context\"].lower()}{sep}{example[\"anchor\"].lower()}{sep}{example[\"target\"].lower()}'\n",
    "    else:\n",
    "        return f'{example[col]}{sep}{example[\"context\"]}{sep}{example[\"anchor\"]}{sep}{example[\"target\"]}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def build_inputs(\n",
    "    example,\n",
    "    sep=\" [s] \",\n",
    "    lower_case=True,\n",
    "):\n",
    "\n",
    "    full_text = example[\"full_text\"].lower() if lower_case else example[\"full_text\"]\n",
    "\n",
    "    inp = full_text\n",
    "    #inp = f\"{context}{sep}{anchor}{sep}{target}\"\n",
    "\n",
    "    return inp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def build_inputs_like_pretrained(example, sep=\" [s] \"):\n",
    "\n",
    "    #return f\"{section}{sep}{context}{sep}{anchor}\", f\"{target}\"\n",
    "    return example[\"full_text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_dls(\n",
    "    df,\n",
    "    hf_arch,\n",
    "    hf_config,\n",
    "    hf_tokenizer,\n",
    "    hf_model,\n",
    "    get_x_func,\n",
    "    val_idxs_or_fold,\n",
    "    max_length=128,\n",
    "    include_labels=True,\n",
    "    tok_kwargs={},\n",
    "    batch_size=4,\n",
    "    seed=utils.default_seed,\n",
    "    augment=\"none\",\n",
    "    sep=\" [s] \",\n",
    "):\n",
    "    # define validation set\n",
    "    if isinstance(val_idxs_or_fold, int):\n",
    "        df[\"is_valid\"] = df[\"k_fold\"] == val_idxs_or_fold\n",
    "        splitter = ColSplitter()\n",
    "    else:\n",
    "        splitter = IndexSplitter(val_idxs_or_fold)\n",
    "\n",
    "    set_seed(seed)\n",
    "    blocks = (\n",
    "        TextBlock(hf_arch, hf_config, hf_tokenizer, hf_model, max_length=max_length, include_labels=include_labels, tok_kwargs=tok_kwargs),\n",
    "        RegressionBlock(n_out = 6),\n",
    "    )\n",
    "\n",
    "    # move this to arg?\n",
    "    target_cols = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
    "\n",
    "    if augment == \"none\":\n",
    "        dblock = DataBlock(blocks=blocks, get_x=ColReader('full_text'), get_y=ColReader(target_cols), splitter=splitter)\n",
    "    elif augment == \"rev\":\n",
    "        print(\"Using augmentation: rev\")\n",
    "        print(\"\")\n",
    "        dblock = DataBlock(\n",
    "            blocks=blocks,\n",
    "            get_x=ColReader('full_text'),\n",
    "            get_y=ColReader(target_cols),\n",
    "            splitter=splitter,\n",
    "            item_tfms=utils.AugmentationTransform(utils.aug, phrase_fct=utils.rev_phrase, sep=sep),\n",
    "        )\n",
    "\n",
    "    set_seed(seed)\n",
    "    return dblock.dataloaders(df, bs=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class YrangeBaseModelWrapper(BaseModelWrapper):\n",
    "    def __init__(self, hf_config, hf_model, y_range=(1.0, 5.0), hf_model_kwargs={}):\n",
    "        super().__init__(\n",
    "            hf_model=hf_model,\n",
    "            output_hidden_states=True,\n",
    "            hf_model_kwargs=hf_model_kwargs,\n",
    "        )\n",
    "        store_attr()\n",
    "\n",
    "        if y_range is not None:\n",
    "            self.y_range = SigmoidRange(*y_range)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = super().forward(x)\n",
    "\n",
    "        if self.y_range:\n",
    "            return self.y_range(res.logits)\n",
    "\n",
    "        return res.logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def yrange_blurr_splitter(m: Module):\n",
    "    base_param_groups = blurr_splitter(m)\n",
    "\n",
    "    added_groups = L([m for m_name, m in list(m.named_children()) if m_name != \"hf_model\"])\n",
    "    base_param_groups[-1] += added_groups.map(params).filter(lambda el: len(el) > 0)\n",
    "\n",
    "    return base_param_groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CustomHeadModelWrapper(BaseModelWrapper):\n",
    "    def __init__(self, hf_config, hf_model, p=0.0, y_range=None, layer_norm_eps=1e-5, hf_model_kwargs={}):\n",
    "        super().__init__(\n",
    "            hf_model=hf_model,\n",
    "            output_hidden_states=True,\n",
    "            hf_model_kwargs=hf_model_kwargs,\n",
    "        )\n",
    "        store_attr()\n",
    "\n",
    "        self.layer_norm1 = nn.LayerNorm(self.hf_config.hidden_size, eps=self.layer_norm_eps)\n",
    "        self.fc_dropout = nn.Dropout(p)\n",
    "        self.fc = nn.Linear(self.hf_config.hidden_size, 1)\n",
    "\n",
    "        if y_range is not None:\n",
    "            self.y_range = SigmoidRange(*y_range)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = super().forward(inputs)\n",
    "        last_hidden_state = outputs[0]\n",
    "\n",
    "        input_mask_expanded = inputs[\"attention_mask\"].unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "\n",
    "        out = sum_embeddings / sum_mask\n",
    "        out = self.layer_norm1(out)\n",
    "        logits = self.fc(self.fc_dropout(out))\n",
    "\n",
    "        if self.y_range:\n",
    "            return self.y_range(logits)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def custom_head_blurr_splitter(m: Module):\n",
    "    base_param_groups = blurr_splitter(m)\n",
    "\n",
    "    added_groups = L([m for m_name, m in list(m.named_children()) if m_name != \"hf_model\"])\n",
    "    added_param_groups = added_groups.map(params).filter(lambda el: len(el) > 0)\n",
    "\n",
    "    return base_param_groups + added_param_groups\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Learner`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_learner(\n",
    "    dls,\n",
    "    hf_model,\n",
    "    custom_base_model=None,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.98,\n",
    "    adam_eps=1e-6,\n",
    "    weight_decay=0.01,\n",
    "    splitter=blurr_splitter,\n",
    "    use_fp16=True,\n",
    "    seed=utils.default_seed,\n",
    "):\n",
    "    # configure the model, loss function and learner\n",
    "    if not custom_base_model:\n",
    "        model = BaseModelWrapper(hf_model)\n",
    "        loss_func = utils.MCRMSELoss()\n",
    "        learn_cbs = [BaseModelCallback]\n",
    "    else:\n",
    "        model = custom_base_model\n",
    "        loss_func = SmoothL1Loss(reduction = \"mean\")\n",
    "        learn_cbs = []\n",
    "\n",
    "    set_seed(seed)\n",
    "\n",
    "    learn = Learner(\n",
    "        dls,\n",
    "        model,\n",
    "        opt_func=partial(Adam, mom=adam_beta1, sqr_mom=adam_beta2, eps=adam_eps, wd=weight_decay),\n",
    "        moms=(adam_beta1, adam_beta1 - 0.10, adam_beta1),\n",
    "        cbs=learn_cbs,\n",
    "        splitter=splitter,\n",
    "    )\n",
    "\n",
    "    learn.loss_func = loss_func\n",
    "    learn.metrics = [utils.MCRMSE()]\n",
    "    learn.create_opt()\n",
    "    learn.freeze()\n",
    "\n",
    "    if use_fp16:\n",
    "        learn = learn.to_fp16()\n",
    "\n",
    "    return learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class FastaiCompTrainer(utils.CompTrainer):\n",
    "    def __init__(self, train_config, model_name, model_output_path=\"models\", log_output_path=\"logs\", **kwargs):\n",
    "        super().__init__(\n",
    "            train_config=train_config, model_name=model_name, model_output_path=model_output_path, log_output_path=log_output_path, **kwargs\n",
    "        )\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        CFG,\n",
    "        data,\n",
    "        experiment_name: str,\n",
    "        n_fold: int,\n",
    "        run_id: str,\n",
    "        grid_id: int,\n",
    "        seed=None,\n",
    "        verbose: bool = True,\n",
    "    ):\n",
    "        # timing\n",
    "        start = time.time()\n",
    "        \n",
    "        if seed is None:\n",
    "            seed = CFG.random_seed\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Experiment: {experiment_name}\")\n",
    "            print(f\"Run: {run_id} | Grid ID: {grid_id} | Fold: {n_fold}\")\n",
    "            print(f\"Training config: f{self.train_config}\")\n",
    "\n",
    "        # from the experiment's training config if not in CFG\n",
    "        # ------\n",
    "        model_checkpoint = self.get_value_for(\"model_checkpoint\", CFG, None)\n",
    "        hf_model_cls = self.get_value_for(\"hf_model_cls\", CFG, AutoModelForSequenceClassification)\n",
    "        using_pretrained = self.get_value_for(\"using_pretrained\", CFG, False)\n",
    "        hf_config_kwargs = self.get_value_for(\"hf_config_kwargs\", CFG, {\"num_labels\": 6})\n",
    "        hf_tokenizer_kwargs = self.get_value_for(\"hf_tokenizer_kwargs\", CFG, {})\n",
    "\n",
    "        # datablock/dataloaders\n",
    "        max_length = self.get_value_for(\"max_length\", CFG, 256)\n",
    "        include_labels = self.get_value_for(\"include_labels\", CFG, True)\n",
    "        tok_kwargs = self.get_value_for(\"tok_kwargs\", CFG, {})\n",
    "        batch_size = self.get_value_for(\"batch_size\", CFG, 4)\n",
    "\n",
    "        # learner\n",
    "        adam_beta1 = self.get_value_for(\"adam_beta1\", CFG, 0.9)\n",
    "        adam_beta2 = self.get_value_for(\"adam_beta2\", CFG, 0.999)\n",
    "        adam_eps = self.get_value_for(\"adam_eps\", CFG, 1e-6)\n",
    "        weight_decay = self.get_value_for(\"weight_decay\", CFG, 0.01)\n",
    "        max_grad_norm = self.get_value_for(\"max_grad_norm\", CFG, None)\n",
    "        splitter = self.get_value_for(\"splitter\", CFG, blurr_splitter)\n",
    "        save_best_model = self.get_value_for(\"save_best_model\", CFG, True)\n",
    "        use_fp16 = self.get_value_for(\"use_fp16\", CFG, True)\n",
    "        custom_base_model_cls = self.get_value_for(\"custom_base_model_cls\", CFG, None)\n",
    "        custom_base_model_kwargs = self.get_value_for(\"custom_base_model_kwargs\", CFG, {})\n",
    "\n",
    "        # training\n",
    "        n_frozen_epochs = self.get_value_for(\"n_frozen_epochs\", CFG, 0)\n",
    "        frozen_lr = self.get_value_for(\"frozen_lr\", CFG, 1e-2)\n",
    "        n_unfrozen_epochs = self.get_value_for(\"n_unfrozen_epochs\", CFG, 5)\n",
    "        unfrozen_lrs = self.get_value_for(\"unfrozen_lrs\", CFG, [1e-5, 1e-2])\n",
    "        # ------\n",
    "\n",
    "        # move this to arg?\n",
    "        target_cols = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
    "\n",
    "        df = data.copy()\n",
    "        #df[\"section_tok\"] = df.section.apply(lambda v: f\"[{v}]\")\n",
    "\n",
    "        # step 1: hf objects\n",
    "        if verbose:\n",
    "            print(\"Building HF objects ...\")\n",
    "\n",
    "        #if not using_pretrained:\n",
    "        #    pass\n",
    "        #    new_special_tokens = list(df.section_tok.unique())\n",
    "        #    new_special_tokens.append(\"[s]\")\n",
    "        #else:\n",
    "        new_special_tokens = None\n",
    "\n",
    "        set_seed(seed)\n",
    "        hf_arch, hf_config, hf_tokenizer, hf_model = get_comp_hf_objects(\n",
    "            model_checkpoint,\n",
    "            model_cls=hf_model_cls,\n",
    "            config_kwargs=hf_config_kwargs,\n",
    "            tokenizer_kwargs=hf_tokenizer_kwargs,\n",
    "            new_special_tokens=new_special_tokens,\n",
    "        )\n",
    "\n",
    "        # step 2: dataloaders\n",
    "        if verbose:\n",
    "            print(\"Building DataLoaders ...\")\n",
    "\n",
    "        if not using_pretrained:\n",
    "            get_x_func = partial(build_inputs)\n",
    "        else:\n",
    "            get_x_func = partial(build_inputs_like_pretrained)\n",
    "\n",
    "        dls = get_dls(\n",
    "            df,\n",
    "            hf_arch,\n",
    "            hf_config,\n",
    "            hf_tokenizer,\n",
    "            hf_model,\n",
    "            get_x_func=get_x_func,\n",
    "            val_idxs_or_fold=n_fold,\n",
    "            max_length=max_length,\n",
    "            include_labels=include_labels,\n",
    "            tok_kwargs=tok_kwargs,\n",
    "            batch_size=batch_size,\n",
    "            seed=seed,\n",
    "            augment=CFG.augment,\n",
    "        )\n",
    "\n",
    "        b = dls.one_batch()\n",
    "        print(f\"b0 shape: {b[0]['input_ids'].shape}; b1 shape: {b[1].shape}\")\n",
    "\n",
    "        # step 3: train\n",
    "        if verbose:\n",
    "            print(\"Building Learner ...\")\n",
    "\n",
    "        base_model_wrapper = None\n",
    "        if custom_base_model_cls:\n",
    "            base_model_wrapper = custom_base_model_cls(hf_config=hf_config, hf_model=hf_model, **custom_base_model_kwargs)\n",
    "\n",
    "        self.learn = get_learner(\n",
    "            dls,\n",
    "            hf_model,\n",
    "            custom_base_model=base_model_wrapper,\n",
    "            adam_beta1=adam_beta1,\n",
    "            adam_beta2=adam_beta2,\n",
    "            adam_eps=adam_eps,\n",
    "            weight_decay=weight_decay,\n",
    "            splitter=splitter,\n",
    "            use_fp16=use_fp16,\n",
    "            seed=seed,\n",
    "        )\n",
    "\n",
    "        fit_cbs = []\n",
    "        if max_grad_norm:\n",
    "            fit_cbs.append(GradientClip(max_norm=max_grad_norm))\n",
    "\n",
    "        if save_best_model:\n",
    "            fit_cbs.append(\n",
    "                SaveModelCallback(\n",
    "                    monitor=\"mcrmse\",\n",
    "                    comp=np.less,\n",
    "                    fname=f\"{self.model_name}_best_mcrsme\",\n",
    "                    reset_on_fit=False,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if n_frozen_epochs > 0:\n",
    "            set_seed(seed)\n",
    "            self.learn.fit_one_cycle(n_frozen_epochs, lr_max=frozen_lr, cbs=fit_cbs)\n",
    "\n",
    "        if n_unfrozen_epochs > 0:\n",
    "            self.learn.unfreeze()\n",
    "            set_seed(seed)\n",
    "            self.learn.fit_one_cycle(n_unfrozen_epochs, lr_max=slice(*unfrozen_lrs), cbs=fit_cbs)\n",
    "\n",
    "        # step 4: log run details and performance on OOF data\n",
    "        log_df = pd.DataFrame(self.learn.recorder.values, columns=self.learn.recorder.metric_names[1:-1])\n",
    "        log_df[\"epoch\"] = log_df.index\n",
    "        log_df[\"fold\"] = n_fold\n",
    "        log_df[\"experiment\"] = experiment_name\n",
    "        log_df[\"run_id\"] = run_id\n",
    "        log_df[\"grid_id\"] = grid_id\n",
    "        log_df[\"subset\"] = CFG.subset\n",
    "        log_df[\"seed\"] = seed\n",
    "        log_df[\"batch_size\"] = batch_size\n",
    "        log_df[\"max_length\"] = max_length\n",
    "        log_df[\"n_frozen_epochs\"] = n_frozen_epochs\n",
    "        log_df[\"frozen_lr\"] = frozen_lr\n",
    "        log_df[\"n_unfrozen_epochs\"] = n_unfrozen_epochs\n",
    "        log_df[\"unfrozen_lrs\"] = str(unfrozen_lrs)[1:-1]\n",
    "        log_df[\"adam_beta1\"] = adam_beta1\n",
    "        log_df[\"adam_beta2\"] = adam_beta2\n",
    "        log_df[\"adam_beta2\"] = adam_beta2\n",
    "        log_df[\"adam_eps\"] = adam_eps\n",
    "        log_df[\"weight_decay\"] = weight_decay\n",
    "        log_df[\"max_grad_norm\"] = max_grad_norm\n",
    "        log_df[\"save_best_model\"] = save_best_model\n",
    "        log_df[\"use_fp16\"] = use_fp16\n",
    "        #log_df[\"strat_feat\"] = CFG.strat_feat\n",
    "        log_df[\"preprocess\"] = CFG.preprocess\n",
    "        log_df[\"postprocess\"] = CFG.postprocess\n",
    "        log_df[\"augment\"] = CFG.augment\n",
    "\n",
    "        # oof df\n",
    "        preds, targs = self.learn.get_preds()\n",
    "        preds = pd.DataFrame(preds)\n",
    "        preds.columns = [\"pred_\" + x for x in target_cols]\n",
    "        targs = pd.DataFrame(targs)\n",
    "        targs.columns = [\"targ_\" + x for x in target_cols]\n",
    "        oof_df = pd.concat([preds, targs], axis = 1)\n",
    "        oof_df[\"run_id\"] = run_id\n",
    "        oof_df[\"grid_id\"] = grid_id\n",
    "\n",
    "        # time keeping\n",
    "        log_df[\"time\"] = time.time() - start\n",
    "\n",
    "        # save model\n",
    "        if verbose:\n",
    "            print(\"--- saving model ---\")\n",
    "\n",
    "        self.learn.metrics = None\n",
    "        self.learn.export(self.model_output_path / f\"{self.model_name}.pkl\")\n",
    "\n",
    "        # clean up\n",
    "        del self.learn, dls\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        return log_df, oof_df\n",
    "\n",
    "    def predict(self, model_name, data):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('kaggle_feedback_ell')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
