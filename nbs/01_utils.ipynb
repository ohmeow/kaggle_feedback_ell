{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |default_exp utils\n",
    "# |default_cls_lvl 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "from __future__ import annotations\n",
    "\n",
    "import abc, datetime, random, os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from fastai.losses import CrossEntropyLossFlat\n",
    "from fastai.test_utils import show_install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import pdb\n",
    "\n",
    "from IPython.display import display\n",
    "from fastcore.test import *\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU #0: GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "# |hide\n",
    "# |cuda\n",
    "gpu_num = int(os.getenv(\"USE_GPU\", 0))\n",
    "\n",
    "torch.cuda.set_device(gpu_num)\n",
    "print(f\"Using GPU #{torch.cuda.current_device()}: {torch.cuda.get_device_name()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defaults\n",
    "\n",
    "Application wide defaults go here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "default_seed = int(os.getenv(\"RANDOM_SEED\", 42))\n",
    "kaggle_comp = os.getenv(\"KAGGLE_COMP\",\"feedback-prize-english-language-learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development environment\n",
    "\n",
    "Information about where your code is running and your compute capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def detect_env():\n",
    "    \"\"\"A helper function that detects where you are running code\"\"\"\n",
    "    if os.environ.get(\"KAGGLE_KERNEL_RUN_TYPE\", False):\n",
    "        run_env = \"kaggle\"\n",
    "    elif os.path.isdir(\"/content\"):\n",
    "        run_env = \"colab\"\n",
    "    elif os.path.isdir(\"../nbs\") or  os.path.isdir(\"../../nbs\"):\n",
    "        run_env = \"local_nb\"\n",
    "    else:\n",
    "        run_env = \"script\"\n",
    "\n",
    "    return run_env\n",
    "\n",
    "\n",
    "run_env = detect_env()\n",
    "\n",
    "if run_env != \"kaggle\":\n",
    "    from kaggle import api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local_nb\n"
     ]
    }
   ],
   "source": [
    "print(run_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def print_dev_environment():\n",
    "    \"\"\"Provides details on your development environment including packages installed, cuda/cudnn availability, GPUs, etc.\"\"\"\n",
    "    print(show_install())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_dev_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Competition Setup\n",
    "\n",
    "**NOTE**: The first thing you should run when setting things up after you've deinfed your `kaggle_comp`, is `setup_comp()`. This method will ensure all the necessary folders are created as well as download the competition data if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_paths(override_project_root=None):\n",
    "    \"\"\"Returns data, models, and log folder paths based on your where you are running the code\"\"\"\n",
    "    if run_env == \"kaggle\":\n",
    "        data_path = Path(\".\")\n",
    "        comp_data_path = clean_data_path= Path(f\"../input/{kaggle_comp}\")\n",
    "        working_path = Path(\"/kaggle/working\")\n",
    "        models_path = working_path / \"models\"\n",
    "        logs_path = working_path / \"logs\"\n",
    "\n",
    "    elif run_env == \"colab\":\n",
    "        proj_root_path = override_project_root or Path(\".\")\n",
    "\n",
    "        data_path = proj_root_path\n",
    "        comp_data_path = clean_data_path = data_path\n",
    "        models_path = data_path / \"models\"\n",
    "        logs_path = data_path / \"logs\"\n",
    "\n",
    "    elif run_env == \"local_nb\":\n",
    "        proj_root_path = override_project_root or Path(\"..\")\n",
    "\n",
    "        data_path = Path(proj_root_path/\"data\")\n",
    "        comp_data_path = data_path / \"comp\"\n",
    "        clean_data_path = data_path / \"clean\"\n",
    "        models_path = Path(proj_root_path/\"models\")\n",
    "        logs_path = Path(proj_root_path/\"logs\")\n",
    "\n",
    "        comp_data_path.mkdir(parents=True, exist_ok=True)\n",
    "        clean_data_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    elif run_env == \"script\":\n",
    "        proj_root_path = override_project_root or Path(\".\")\n",
    "\n",
    "        data_path = Path(proj_root_path/\"data\")\n",
    "        comp_data_path = data_path / \"comp\"\n",
    "        clean_data_path = data_path / \"clean\"\n",
    "        models_path = Path(proj_root_path/\"models\")\n",
    "        logs_path = Path(proj_root_path/\"logs\")\n",
    "\n",
    "        comp_data_path.mkdir(parents=True, exist_ok=True)\n",
    "        clean_data_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        models_path.mkdir(parents=True, exist_ok=True)\n",
    "        logs_path.mkdir(parents=True, exist_ok=True)\n",
    "    except:\n",
    "        print(\"Unable to create models and logs folders\")\n",
    "        \n",
    "    return data_path, comp_data_path, clean_data_path, models_path, logs_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def setup_comp(override_project_root=None, comp_data_path_override=None):\n",
    "    \"\"\"Ensures that the expected data, models, and logs folders exist and that the competition data exists in the 'comp_data_path'.\"\"\"\n",
    "\n",
    "    if comp_data_path_override is not None:\n",
    "        comp_data_path = comp_data_path_override\n",
    "    else:\n",
    "        _, comp_data_path, *_ = get_paths(override_project_root)\n",
    "\n",
    "    if run_env != \"kaggle\":\n",
    "        if not comp_data_path.exists() or not any(comp_data_path.iterdir()):\n",
    "            import zipfile\n",
    "\n",
    "            api.competition_download_cli(kaggle_comp)\n",
    "\n",
    "            zipfile.ZipFile(f\"{kaggle_comp}.zip\").extractall(comp_data_path)\n",
    "            Path(f\"{kaggle_comp}.zip\").unlink(missing_ok=True)\n",
    "\n",
    "        return comp_data_path\n",
    "    else:\n",
    "        return Path(f\"../input/{kaggle_comp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('../data/comp')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setup_comp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Competition Metrics\n",
    "\n",
    "This competition is evaluated using \"multi-class logarithmic loss\" (e.g., cross-entropy loss). From the competition website ...\n",
    "\n",
    "> Each row in the dataset has been labeled with one true effectiveness label. For each row, you must submit the predicted probabilities that the product belongs to each quality label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def comp_metric_score(preds, targs):\n",
    "    \"\"\"This competition is evaluated using \"multi-class logarithmic loss\" (e.g., cross-entropy loss). Expects numpy arrays.\"\"\"\n",
    "    probs = np.exp(preds) / np.sum(np.exp(preds), axis=1, keepdims=True)\n",
    "\n",
    "    correct_class_probs = probs[range(len(preds)), targs]\n",
    "    nll = -np.log(correct_class_probs)\n",
    "    return nll.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify our comp_metric_score calculates cross-entropy loss correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2828,  0.1889, -0.0737, -0.4460, -0.0887],\n",
      "        [ 0.3788,  2.0066,  0.0591, -0.7007,  0.3658],\n",
      "        [-0.8615,  0.4423, -0.3266,  0.2098, -0.1276]])\n",
      "tensor([0, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "preds = torch.randn(3, 5)\n",
    "targets = torch.tensor([0, 3, 4])\n",
    "\n",
    "print(preds)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.0688), TensorBase(2.0688), 2.0688422)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_loss = F.cross_entropy(preds, targets)\n",
    "fastai_loss = CrossEntropyLossFlat()(preds, targets)\n",
    "np_loss = comp_metric_score(preds.numpy(), targets.numpy())\n",
    "\n",
    "test_close(pytorch_loss.item(), fastai_loss.item())\n",
    "test_close(pytorch_loss.item(), np_loss)\n",
    "\n",
    "pytorch_loss, fastai_loss, np_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_run_id():\n",
    "    run_id = str(datetime.datetime.now())[:16].replace(\":\", \"_\").replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "    return run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022_09_13_20_38\n"
     ]
    }
   ],
   "source": [
    "print(get_run_id())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('kaggle_feedback_ell')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
