{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp framework_utils.fastai_cocolm\n",
    "#| default_cls_lvl 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# framework_utils.fastai_cocolm\n",
    "\n",
    "> As Jeremy says in his notebook, \"Iteration speed is critical, so we need to quickly be able to try different data processing and trainer parameters\" ... so lets create some helper methods to help us iterate more quickly.\n",
    "\n",
    "This notebook is specific to MS coco-lm due to its lack of proper integration with HF transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os, gc, time\n",
    "\n",
    "from fastai.callback.all import *\n",
    "from fastai.data.block import ColReader, ColSplitter, DataBlock, IndexSplitter, RegressionBlock\n",
    "from fastai.imports import *\n",
    "from fastai.layers import SigmoidRange\n",
    "from fastai.learner import *\n",
    "from fastai.losses import MSELossFlat\n",
    "from fastai.optimizer import Adam\n",
    "from fastai.metrics import *\n",
    "from fastai.torch_core import *\n",
    "from fastai.torch_imports import *\n",
    "from transformers import AutoConfig, AutoModelForMaskedLM, AutoModelForSequenceClassification, logging\n",
    "\n",
    "from blurr.callbacks import GradientCheckpointing\n",
    "from blurr.text.data.core import TextBlock\n",
    "from blurr.text.modeling.core import BaseModelWrapper, BaseModelCallback, blurr_splitter\n",
    "from blurr.text.utils import get_hf_objects\n",
    "from blurr.utils import PreCalculatedMSELoss, set_seed\n",
    "\n",
    "from kaggle_comp import utils\n",
    "from kaggle_comp.framework_utils import fastai\n",
    "from kaggle_comp.cocolm.modeling_cocolm import COCOLMModel, COCOLMForSequenceClassification\n",
    "from kaggle_comp.cocolm.configuration_cocolm import COCOLMConfig\n",
    "from kaggle_comp.cocolm.tokenization_cocolm import COCOLMTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide_input\n",
    "import pdb\n",
    "from fastcore.test import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU #0: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "#| cuda\n",
    "torch.cuda.set_device(0)\n",
    "print(f\"Using GPU #{torch.cuda.current_device()}: {torch.cuda.get_device_name()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face `transformers` objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_comp_hf_objects(\n",
    "    model_checkpoint,\n",
    "    model_cls=AutoModelForSequenceClassification,\n",
    "    config_kwargs={\"num_labels\": 6},\n",
    "    tokenizer_kwargs={},\n",
    "    model_kwargs={},\n",
    "    new_special_tokens=None,\n",
    "):\n",
    "\n",
    "    # need to create configuration object separately because we may be adding new attributes (e.g., cls_dropout)\n",
    "    hf_arch = \"cocolm\"\n",
    "    hf_config = COCOLMConfig.from_pretrained(\"microsoft/cocolm-base\", **config_kwargs)\n",
    "    hf_tokenizer = COCOLMTokenizer.from_pretrained(\"microsoft/cocolm-base\", **tokenizer_kwargs)\n",
    "    hf_model= COCOLMForSequenceClassification.from_pretrained(\"microsoft/cocolm-base\", config=hf_config, **model_kwargs)\n",
    "\n",
    "    if new_special_tokens:\n",
    "        # After adding the new tokens, we need to resize the embedding matrix in the model and initialize the weights\n",
    "        hf_tokenizer.add_special_tokens({\"additional_special_tokens\": new_special_tokens})\n",
    "        hf_model.resize_token_embeddings(len(hf_tokenizer))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            emb_size = hf_model.config.to_dict().get(\"embedding_size\", hf_model.config.hidden_size )\n",
    "            hf_model.get_input_embeddings().weight[-len(hf_tokenizer), :] = torch.zeros([emb_size])\n",
    "\n",
    "    return hf_arch, hf_config, hf_tokenizer, hf_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CocoLmBaseModelWrapper(Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        # Your Hugging Face model\n",
    "        hf_model,\n",
    "        # If True, hidden_states will be returned and accessed from Learner\n",
    "        output_hidden_states: bool = False,\n",
    "        # If True, attentions will be returned and accessed from Learner\n",
    "        output_attentions: bool = False,\n",
    "        # Any additional keyword arguments you want passed into your models forward method\n",
    "        hf_model_kwargs={},\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        store_attr()\n",
    "        self.hf_model = hf_model.cuda() if torch.cuda.is_available() else hf_model\n",
    "        self.hf_model_fwd_args = list(inspect.signature(self.hf_model.forward).parameters.keys())\n",
    "\n",
    "    def forward(self, x):\n",
    "        for k in list(x):\n",
    "            if k not in self.hf_model_fwd_args:\n",
    "                del x[k]\n",
    "\n",
    "        res = self.hf_model(\n",
    "            **x,\n",
    "             **self.hf_model_kwargs\n",
    "        )\n",
    "\n",
    "        if len(res) > 1:\n",
    "            return {\"loss\": res[0], \"logits\": res[1]}\n",
    "        else:\n",
    "            return {\"logits\": res[0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Learner`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_learner(\n",
    "    dls,\n",
    "    hf_model,\n",
    "    custom_base_model=None,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.98,\n",
    "    adam_eps=1e-6,\n",
    "    weight_decay=0.01,\n",
    "    splitter=blurr_splitter,\n",
    "    use_fp16=True,\n",
    "    seed=utils.default_seed,\n",
    "):\n",
    "    set_seed(seed)\n",
    "\n",
    "    model = CocoLmBaseModelWrapper(hf_model)\n",
    "    loss_func = utils.MCRMSELoss()\n",
    "    learn_cbs = [BaseModelCallback]\n",
    "\n",
    "    learn = Learner(\n",
    "        dls,\n",
    "        model,\n",
    "        opt_func=partial(Adam, mom=adam_beta1, sqr_mom=adam_beta2, eps=adam_eps, wd=weight_decay),\n",
    "        moms=(adam_beta1, adam_beta1 - 0.10, adam_beta1),\n",
    "        cbs=learn_cbs,\n",
    "        splitter=splitter,\n",
    "    )\n",
    "\n",
    "    learn.loss_func = loss_func\n",
    "    learn.metrics = [utils.MCRMSE()]\n",
    "    learn.create_opt()\n",
    "    learn.freeze()\n",
    "\n",
    "    if use_fp16:\n",
    "        learn = learn.to_fp16()\n",
    "\n",
    "    return learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class FastaiCocoLmCompTrainer(utils.CompTrainer):\n",
    "    def __init__(self, train_config, model_name, model_output_path=\"models\", log_output_path=\"logs\", **kwargs):\n",
    "        super().__init__(\n",
    "            train_config=train_config, model_name=model_name, model_output_path=model_output_path, log_output_path=log_output_path, **kwargs\n",
    "        )\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        CFG,\n",
    "        data,\n",
    "        experiment_name: str,\n",
    "        n_fold: int,\n",
    "        run_id: str,\n",
    "        grid_id: int,\n",
    "        seed=None,\n",
    "        verbose: bool = True,\n",
    "    ):\n",
    "        # timing\n",
    "        start = time.time()\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Experiment: {experiment_name}\")\n",
    "            print(f\"Run: {run_id} | Grid ID: {grid_id} | Fold: {n_fold}\")\n",
    "            print(f\"Training config: f{self.train_config}\")\n",
    "\n",
    "        if seed is None:\n",
    "            seed = CFG.random_seed\n",
    "\n",
    "        # from the experiment's training config if not in CFG\n",
    "        # ------\n",
    "        model_checkpoint = self.train_config.get(\"model_checkpoint\", None)\n",
    "        using_pretrained = self.train_config.get(\"using_pretrained\", False)\n",
    "        hf_config_kwargs = self.get_value_for(\"hf_config_kwargs\", CFG, {\"num_labels\": 6})\n",
    "        hf_tokenizer_kwargs = self.get_value_for(\"hf_tokenizer_kwargs\", CFG, {})\n",
    "\n",
    "       # datablock/dataloaders\n",
    "        max_length = self.get_value_for(\"max_length\", CFG, 256)\n",
    "        include_labels = self.get_value_for(\"include_labels\", CFG, True)\n",
    "        tok_kwargs = self.get_value_for(\"tok_kwargs\", CFG, {})\n",
    "        batch_size = self.get_value_for(\"batch_size\", CFG, 4)\n",
    "\n",
    "        # learner\n",
    "        adam_beta1 = self.get_value_for(\"adam_beta1\", CFG, 0.9)\n",
    "        adam_beta2 = self.get_value_for(\"adam_beta2\", CFG, 0.999)\n",
    "        adam_eps = self.get_value_for(\"adam_eps\", CFG, 1e-6)\n",
    "        weight_decay = self.get_value_for(\"weight_decay\", CFG, 0.01)\n",
    "        max_grad_norm = self.get_value_for(\"max_grad_norm\", CFG, None)\n",
    "        splitter = self.get_value_for(\"splitter\", CFG, blurr_splitter)\n",
    "        save_best_model = self.get_value_for(\"save_best_model\", CFG, True)\n",
    "        use_fp16 = self.get_value_for(\"use_fp16\", CFG, True)\n",
    "        custom_base_model_cls = self.get_value_for(\"custom_base_model_cls\", CFG, None)\n",
    "        custom_base_model_kwargs = self.get_value_for(\"custom_base_model_kwargs\", CFG, {})\n",
    "\n",
    "        # training\n",
    "        n_frozen_epochs = self.get_value_for(\"n_frozen_epochs\", CFG, 0)\n",
    "        frozen_lr = self.get_value_for(\"frozen_lr\", CFG, 1e-2)\n",
    "        n_unfrozen_epochs = self.get_value_for(\"n_unfrozen_epochs\", CFG, 5)\n",
    "        unfrozen_lrs = self.get_value_for(\"unfrozen_lrs\", CFG, [1e-5, 1e-2])\n",
    "        # ------\n",
    "        \n",
    "        target_cols = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
    "\n",
    "        df = data.copy()\n",
    "        \n",
    "        # step 1: hf objects\n",
    "        if verbose:\n",
    "            print(\"Building HF objects ...\")\n",
    "\n",
    "        # WTG: Can't figure out how to add tokens to COCOLM\n",
    "        new_special_tokens = None\n",
    "\n",
    "        set_seed(seed)\n",
    "        hf_arch, hf_config, hf_tokenizer, hf_model = get_comp_hf_objects(\n",
    "            model_checkpoint, config_kwargs=hf_config_kwargs, tokenizer_kwargs=hf_tokenizer_kwargs, new_special_tokens=new_special_tokens\n",
    "        )\n",
    "\n",
    "        # step 2: dataloaders\n",
    "        if verbose:\n",
    "            print(\"Building DataLoaders ...\")\n",
    "\n",
    "        if not using_pretrained:\n",
    "            get_x_func = partial(fastai.build_inputs)\n",
    "        else:\n",
    "            get_x_func = partial(fastai.build_inputs_like_pretrained)\n",
    "            \n",
    "        dls = fastai.get_dls(\n",
    "            df,\n",
    "            hf_arch,\n",
    "            hf_config,\n",
    "            hf_tokenizer,\n",
    "            hf_model,\n",
    "            get_x_func=get_x_func,\n",
    "            val_idxs_or_fold=n_fold,\n",
    "            max_length=max_length,\n",
    "            include_labels=include_labels,\n",
    "            tok_kwargs=tok_kwargs,\n",
    "            batch_size=batch_size,\n",
    "            seed=seed,\n",
    "            augment=CFG.augment\n",
    "        )\n",
    "\n",
    "        # step 3: train\n",
    "        if verbose:\n",
    "            print(\"Building Learner ...\")\n",
    "\n",
    "        base_model_wrapper = None\n",
    "        if custom_base_model_cls:\n",
    "            base_model_wrapper = custom_base_model_cls(hf_config=hf_config, hf_model=hf_model, **custom_base_model_kwargs)\n",
    "\n",
    "        self.learn = get_learner(\n",
    "            dls,\n",
    "            hf_model,\n",
    "            custom_base_model=base_model_wrapper,\n",
    "            adam_beta1=adam_beta1,\n",
    "            adam_beta2=adam_beta2,\n",
    "            adam_eps=adam_eps,\n",
    "            weight_decay=weight_decay,\n",
    "            splitter=splitter,\n",
    "            use_fp16=use_fp16,\n",
    "            seed=seed,\n",
    "        )\n",
    "\n",
    "        fit_cbs = []\n",
    "        if max_grad_norm:\n",
    "            fit_cbs.append(GradientClip(max_norm=max_grad_norm))\n",
    "\n",
    "        if save_best_model:\n",
    "            fit_cbs.append(\n",
    "                SaveModelCallback(\n",
    "                    monitor=\"mcrmse\",\n",
    "                    comp=np.less,\n",
    "                    fname=f\"{self.model_name}_best_mcrsme\",\n",
    "                    reset_on_fit=False,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if n_frozen_epochs > 0:\n",
    "            set_seed(seed)\n",
    "            self.learn.fit_one_cycle(n_frozen_epochs, lr_max=frozen_lr, cbs=fit_cbs)\n",
    "\n",
    "        if n_unfrozen_epochs > 0:\n",
    "            self.learn.unfreeze()\n",
    "            set_seed(seed)\n",
    "            self.learn.fit_one_cycle(n_unfrozen_epochs, lr_max=slice(*unfrozen_lrs), cbs=fit_cbs)\n",
    "\n",
    "        # step 4: log run details and performance on OOF data\n",
    "        log_df = pd.DataFrame(self.learn.recorder.values, columns=self.learn.recorder.metric_names[1:-1])\n",
    "        log_df[\"epoch\"] = log_df.index\n",
    "        log_df[\"fold\"] = n_fold\n",
    "        log_df[\"experiment\"] = experiment_name\n",
    "        log_df[\"run_id\"] = run_id\n",
    "        log_df[\"grid_id\"] = grid_id\n",
    "        log_df[\"subset\"] = CFG.subset\n",
    "        log_df[\"seed\"] = seed\n",
    "        log_df[\"batch_size\"] = batch_size\n",
    "        log_df[\"max_length\"] = max_length\n",
    "        log_df[\"n_frozen_epochs\"] = n_frozen_epochs\n",
    "        log_df[\"frozen_lr\"] = frozen_lr\n",
    "        log_df[\"n_unfrozen_epochs\"] = n_unfrozen_epochs\n",
    "        log_df[\"unfrozen_lrs\"] = str(unfrozen_lrs)[1:-1]\n",
    "        log_df[\"adam_beta1\"] = adam_beta1\n",
    "        log_df[\"adam_beta2\"] = adam_beta2\n",
    "        log_df[\"adam_beta2\"] = adam_beta2\n",
    "        log_df[\"adam_eps\"] = adam_eps\n",
    "        log_df[\"weight_decay\"] = weight_decay\n",
    "        log_df[\"max_grad_norm\"] = max_grad_norm\n",
    "        log_df[\"save_best_model\"] = save_best_model\n",
    "        log_df[\"use_fp16\"] = use_fp16\n",
    "        #log_df[\"strat_feat\"] = CFG.strat_feat\n",
    "        log_df[\"preprocess\"] = CFG.preprocess\n",
    "        log_df[\"postprocess\"] = CFG.postprocess\n",
    "        log_df[\"augment\"] = CFG.augment\n",
    "\n",
    "        # oof df\n",
    "        preds, targs = self.learn.get_preds()\n",
    "        preds = pd.DataFrame(preds)\n",
    "        preds.columns = [\"pred_\" + x for x in target_cols]\n",
    "        targs = pd.DataFrame(targs)\n",
    "        targs.columns = [\"targ_\" + x for x in target_cols]\n",
    "        oof_df = pd.concat([preds, targs], axis = 1)\n",
    "        oof_df[\"run_id\"] = run_id\n",
    "        oof_df[\"grid_id\"] = grid_id\n",
    "\n",
    "        # time keeping\n",
    "        log_df[\"time\"] = time.time() - start\n",
    "\n",
    "        # save model\n",
    "        if verbose:\n",
    "            print(\"--- saving model ---\")\n",
    "\n",
    "        self.learn.metrics = None\n",
    "        self.learn.export(self.model_output_path / f\"{self.model_name}.pkl\")\n",
    "\n",
    "        # clean up\n",
    "        del self.learn, dls\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        return log_df, oof_df\n",
    "\n",
    "    def predict(self, model_name, data):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('kaggle_feedback_ell')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
