# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/04_train_dispatcher.ipynb.

# %% auto 0
__all__ = ['experiments']

# %% ../nbs/04_train_dispatcher.ipynb 3
from transformers import DebertaV2Model

from .framework_utils import fastai
#from kaggle_comp.framework_utils import fastai_cocolm

# %% ../nbs/04_train_dispatcher.ipynb 5
experiments = {
    # deberta-v3
    "blurr_deberta_v3_small": {
        "comp_trainer_cls": fastai.FastaiCompTrainer,
        # hf objects
        "model_checkpoint": "microsoft/deberta-v3-small",
        "hf_config_kwargs": {
            "num_labels": 6,
            "cls_dropout": 0.15,  # default = hidden_dropout_prob
            "pooler_dropout": 0.15,  # default = 0.0
            "hidden_dropout_prob": 0.05,  # default = 0.1
            "attention_probs_dropout_prob": 0.1,  # default = 0.1
            "layer_norm_eps": 1e-5 # default = 1e-7
        },
        "hf_tokenizer_kwargs": {},
        # data
        "anchor_col": "anchor",
        "target_col": "target",
        "max_length": 140,
        "include_labels": True,
        "tok_kwargs": {},
        "batch_size": 128,
        # learner
        "adam_beta1": 0.9,
        "adam_beta2": 0.995,
        "adam_eps": 1e-6,
        "weight_decay": 0.01,
        "max_grad_norm": None,
        "save_best_model": True,
        "use_fp16": True,
        # training
        "n_frozen_epochs": 0,
        "frozen_lr": None,
        "n_unfrozen_epochs": 10,
        "unfrozen_lrs": [1e-5, 2e-2],
    },
    "blurr_deberta_v3_small_yrange": {
        "comp_trainer_cls": fastai.FastaiCompTrainer,
        # hf objects
        "model_checkpoint": "microsoft/deberta-v3-small",
        "hf_config_kwargs": {
            "num_labels": 6,
            "cls_dropout": 0.15,  # default = hidden_dropout_prob
            "pooler_dropout": 0.00,  # default = 0.0
            "hidden_dropout_prob": 0.1,  # default = 0.1
            "attention_probs_dropout_prob": 0.1,  # default = 0.1
            "layer_norm_eps": 1e-7 # default = 1e-7
        },
        "hf_tokenizer_kwargs": {},
        # data
        "anchor_col": "anchor",
        "target_col": "target",
        "max_length": 140,
        "include_labels": False,
        "tok_kwargs": {},
        "batch_size": 128,
        # learner
        "adam_beta1": 0.9,
        "adam_beta2": 0.999,
        "adam_eps": 1e-6,
        "weight_decay": 0.01,
        "max_grad_norm": 1.0,
        "save_best_model": True,
        "use_fp16": True,
        "custom_base_model_cls": fastai.YrangeBaseModelWrapper,
        "custom_base_model_kwargs": {"y_range": (0.0, 1.2)},
        "splitter": fastai.yrange_blurr_splitter,
        # training
        "n_frozen_epochs": 0,
        "frozen_lr": None,
        "n_unfrozen_epochs": 10,
        "unfrozen_lrs": [1e-5, 1e-2],
    },
     "blurr_deberta_v3_small_custom_head": {
        "comp_trainer_cls": fastai.FastaiCompTrainer,
        # hf objects
        "model_checkpoint": "microsoft/deberta-v3-small",
        "hf_model_cls": DebertaV2Model,
        "hf_config_kwargs": {
            "hidden_dropout_prob": 0.0,  # default = 0.1
            "attention_probs_dropout_prob": 0.1,  # default = 0.1
            "layer_norm_eps": 1e-7 # default = 1e-7
        },
        "hf_tokenizer_kwargs": {},
        # data
        "anchor_col": "anchor",
        "target_col": "target",
        "max_length": 140,
        "include_labels": False,
        "tok_kwargs": {},
        "batch_size": 128,
        # learner
        "adam_beta1": 0.9,
        "adam_beta2": 0.999,
        "adam_eps": 1e-5,
        "weight_decay": 0.01,
        "max_grad_norm": None,
        "save_best_model": True,
        "use_fp16": True,
        "custom_base_model_cls": fastai.CustomHeadModelWrapper,
        "custom_base_model_kwargs": {"p": 0.2, "y_range": None, "layer_norm_eps": 1e-6}, # defaults => {"p": 0.0, "y_range": None, "layer_norm_eps": 1e-5}
        "splitter": fastai.custom_head_blurr_splitter,
        # training
        "n_frozen_epochs": 0,
        "frozen_lr": None,
        "n_unfrozen_epochs": 10,
        "unfrozen_lrs": [1.07e-5, 1.57e-2],
    },
    "blurr_deberta_v3_small_st": {
        "comp_trainer_cls": fastai.FastaiCompTrainer,
        "using_pretrained": True,
        # hf objects
        "model_checkpoint": "./models/deberta-v3-small-sts-patent",
        "hf_config_kwargs": {
            "num_labels": 6,
            "cls_dropout": 0.15,  # default = hidden_dropout_prob
            "pooler_dropout": 0.00,  # default = 0.0
            "hidden_dropout_prob": 0.1,  # default = 0.1
            "attention_probs_dropout_prob": 0.1,  # default = 0.1
            "layer_norm_eps": 1e-7 # default = 1e-7
        },
        "hf_tokenizer_kwargs": {},
        # data
        "anchor_col": "anchor",
        "target_col": "target",
        "max_length": 140,
        "include_labels": True,
        "tok_kwargs": {},
        "batch_size": 128,
        # learner
        "adam_beta1": 0.9,
        "adam_beta2": 0.999,
        "adam_eps": 1e-6,
        "weight_decay": 0.01,
        "max_grad_norm": 1.0,
        "save_best_model": True,
        "use_fp16": True,
        # training
        "n_frozen_epochs": 0,
        "frozen_lr": None,
        "n_unfrozen_epochs": 10,
        "unfrozen_lrs": [1e-5, 1e-2],
    },
    "blurr_deberta_v3_base": {
        "comp_trainer_cls": fastai.FastaiCompTrainer,
        # hf objects
        "model_checkpoint": "microsoft/deberta-v3-base",
        "hf_config_kwargs": {
            "num_labels": 6,
            "cls_dropout": 0.15,  # default = hidden_dropout_prob
            "pooler_dropout": 0.15,  # default = 0.0
            "hidden_dropout_prob": 0.05,  # default = 0.1
            "attention_probs_dropout_prob": 0.1,  # default = 0.1
            "layer_norm_eps": 1e-5 # default = 1e-7
        },
        "hf_tokenizer_kwargs": {},
        # data
        "anchor_col": "anchor",
        "target_col": "target",
        "max_length": 140,
        "include_labels": True,
        "tok_kwargs": {},
        "batch_size": 32,
        # learner
        "adam_beta1": 0.9,
        "adam_beta2": 0.995,
        "adam_eps": 1e-6,
        "weight_decay": 0.01,
        "max_grad_norm": None,
        "save_best_model": True,
        "use_fp16": True,
        # training
        "n_frozen_epochs": 0,
        "frozen_lr": None,
        "n_unfrozen_epochs": 10,
        "unfrozen_lrs": [1e-5, 2e-2],
    },
    "blurr_deberta_v3_large": {
        "comp_trainer_cls": fastai.FastaiCompTrainer,
        # hf objects
        "model_checkpoint": "microsoft/deberta-v3-large",
        "hf_config_kwargs": {
            "num_labels": 6,
            "cls_dropout": 0.15,  # default = hidden_dropout_prob
            "pooler_dropout": 0.00,  # default = 0.0
            "hidden_dropout_prob": 0.1,  # default = 0.1
            "attention_probs_dropout_prob": 0.1,  # default = 0.1
            "layer_norm_eps": 1e-7 # default = 1e-7
        },
        "hf_tokenizer_kwargs": {},
        # data
        "anchor_col": "anchor",
        "target_col": "target",
        "max_length": 140,
        "include_labels": True,
        "tok_kwargs": {},
        "batch_size": 8,
        # learner
        "adam_beta1": 0.9,
        "adam_beta2": 0.999,
        "adam_eps": 1e-6,
        "weight_decay": 0.01,
        "max_grad_norm": 1.0,
        "save_best_model": True,
        "use_fp16": True,
        # training
        "n_frozen_epochs": 0,
        "frozen_lr": None,
        "n_unfrozen_epochs": 10,
        "unfrozen_lrs": [1e-6, 2e-5],
    },
    "blurr_deberta_v3_large_custom_head": {
        "comp_trainer_cls": fastai.FastaiCompTrainer,
        # hf objects
        "model_checkpoint": "microsoft/deberta-v3-large",
        "hf_model_cls": DebertaV2Model,
        "hf_config_kwargs": {
            "hidden_dropout_prob": 0.0,  # default = 0.1
            "attention_probs_dropout_prob": 0.1,  # default = 0.1
            "layer_norm_eps": 1e-7 # default = 1e-7
        },
        "hf_tokenizer_kwargs": {},
        # data
        "anchor_col": "anchor",
        "target_col": "target",
        "max_length": 140,
        "include_labels": False,
        "tok_kwargs": {},
        "batch_size": 16,
        # learner
        "adam_beta1": 0.9,
        "adam_beta2": 0.999,
        "adam_eps": 1e-5,
        "weight_decay": 0.01,
        "max_grad_norm": None,
        "save_best_model": True,
        "use_fp16": True,
        "custom_base_model_cls": fastai.CustomHeadModelWrapper,
        "custom_base_model_kwargs": {"p": 0.2, "y_range": None, "layer_norm_eps": 1e-6}, # defaults => {"p": 0.0, "y_range": None, "layer_norm_eps": 1e-5}
        "splitter": fastai.custom_head_blurr_splitter,
        # training
        "n_frozen_epochs": 0,
        "frozen_lr": None,
        "n_unfrozen_epochs": 10,
        "unfrozen_lrs": [1e-6, 2e-5],
    },
    # deberta
    "blurr_deberta_large": {
        "comp_trainer_cls": fastai.FastaiCompTrainer,
        # hf objects
        "model_checkpoint": "microsoft/deberta-large",
         "hf_config_kwargs": {
            "num_labels": 6,
            "cls_dropout": 0.15,  # default = hidden_dropout_prob
            "pooler_dropout": 0.00,  # default = 0.0
            "hidden_dropout_prob": 0.1,  # default = 0.1
            "attention_probs_dropout_prob": 0.1,  # default = 0.1
            "layer_norm_eps": 1e-7 # default = 1e-7
        },
        "hf_tokenizer_kwargs": {},
        # data
        "anchor_col": "anchor",
        "target_col": "target",
        "max_length": 140,
        "include_labels": True,
        "tok_kwargs": {},
        "batch_size": 8,
        # learner
        "adam_beta1": 0.9,
        "adam_beta2": 0.999,
        "adam_eps": 1e-6,
        "weight_decay": 0.01,
        "max_grad_norm": 1.0,
        "save_best_model": True,
        "use_fp16": True,
        # training
        "n_frozen_epochs": 0,
        "frozen_lr": None,
        "n_unfrozen_epochs": 10,
        "unfrozen_lrs": [1e-6, 2e-5],
    },
    # deberta
    "blurr_deberta_xlarge": {
        "comp_trainer_cls": fastai.FastaiCompTrainer,
        # hf objects
        "model_checkpoint": "microsoft/deberta-xlarge",
         "hf_config_kwargs": {
            "num_labels": 6,
            "cls_dropout": 0.15,  # default = hidden_dropout_prob
            "pooler_dropout": 0.00,  # default = 0.0
            "hidden_dropout_prob": 0.1,  # default = 0.1
            "attention_probs_dropout_prob": 0.1,  # default = 0.1
            "layer_norm_eps": 1e-7 # default = 1e-7
        },
        "hf_tokenizer_kwargs": {},
        # data
        "anchor_col": "anchor",
        "target_col": "target",
        "max_length": 140,
        "include_labels": True,
        "tok_kwargs": {},
        "batch_size": 8,
        # learner
        "adam_beta1": 0.9,
        "adam_beta2": 0.999,
        "adam_eps": 1e-6,
        "weight_decay": 0.01,
        "max_grad_norm": 1.0,
        "save_best_model": True,
        "use_fp16": True,
        # training
        "n_frozen_epochs": 0,
        "frozen_lr": None,
        "n_unfrozen_epochs": 10,
        "unfrozen_lrs": [1e-6, 2e-5],
    },
    # deberta
    "blurr_deberta_large_mnli": {
        "comp_trainer_cls": fastai.FastaiCompTrainer,
        # hf objects
        "model_checkpoint": "microsoft/deberta-large-mnli",
         "hf_config_kwargs": {
            "num_labels": 6,
            "cls_dropout": 0.15,  # default = hidden_dropout_prob
            "pooler_dropout": 0.00,  # default = 0.0
            "hidden_dropout_prob": 0.1,  # default = 0.1
            "attention_probs_dropout_prob": 0.1,  # default = 0.1
            "layer_norm_eps": 1e-7 # default = 1e-7
        },
        "hf_tokenizer_kwargs": {},
        # data
        "anchor_col": "anchor",
        "target_col": "target",
        "max_length": 140,
        "include_labels": True,
        "tok_kwargs": {},
        "batch_size": 8,
        # learner
        "adam_beta1": 0.9,
        "adam_beta2": 0.999,
        "adam_eps": 1e-6,
        "weight_decay": 0.01,
        "max_grad_norm": 1.0,
        "save_best_model": True,
        "use_fp16": True,
        # training
        "n_frozen_epochs": 0,
        "frozen_lr": None,
        "n_unfrozen_epochs": 10,
        "unfrozen_lrs": [1e-6, 2e-5],
    },
    "blurr_deberta_xlarge_mnli": {
        "comp_trainer_cls": fastai.FastaiCompTrainer,
        # hf objects
        "model_checkpoint": "microsoft/deberta-xlarge-mnli",
         "hf_config_kwargs": {
            "num_labels": 6,
            "cls_dropout": 0.15,  # default = hidden_dropout_prob
            "pooler_dropout": 0.00,  # default = 0.0
            "hidden_dropout_prob": 0.1,  # default = 0.1
            "attention_probs_dropout_prob": 0.1,  # default = 0.1
            "layer_norm_eps": 1e-7 # default = 1e-7
        },
        "hf_tokenizer_kwargs": {},
        # data
        "anchor_col": "anchor",
        "target_col": "target",
        "max_length": 140,
        "include_labels": True,
        "tok_kwargs": {},
        "batch_size": 8,
        # learner
        "adam_beta1": 0.9,
        "adam_beta2": 0.999,
        "adam_eps": 1e-6,
        "weight_decay": 0.01,
        "max_grad_norm": 1.0,
        "save_best_model": True,
        "use_fp16": True,
        # training
        "n_frozen_epochs": 0,
        "frozen_lr": None,
        "n_unfrozen_epochs": 10,
        "unfrozen_lrs": [1e-6, 2e-5],
    },
    "blurr_deberta_large_custom_head": {
        "comp_trainer_cls": fastai.FastaiCompTrainer,
        # hf objects
        "model_checkpoint": "microsoft/deberta-large",
        "hf_model_cls": DebertaV2Model,
        "hf_config_kwargs": {
            "hidden_dropout_prob": 0.0,  # default = 0.1
            "attention_probs_dropout_prob": 0.1,  # default = 0.1
            "layer_norm_eps": 1e-7 # default = 1e-7
        },
        "hf_tokenizer_kwargs": {},
        # data
        "anchor_col": "anchor",
        "target_col": "target",
        "max_length": 140,
        "include_labels": False,
        "tok_kwargs": {},
        "batch_size": 16,
        # learner
        "adam_beta1": 0.9,
        "adam_beta2": 0.999,
        "adam_eps": 1e-5,
        "weight_decay": 0.01,
        "max_grad_norm": None,
        "save_best_model": True,
        "use_fp16": True,
        "custom_base_model_cls": fastai.CustomHeadModelWrapper,
        "custom_base_model_kwargs": {"p": 0.2, "y_range": None, "layer_norm_eps": 1e-6}, # defaults => {"p": 0.0, "y_range": None, "layer_norm_eps": 1e-5}
        "splitter": fastai.custom_head_blurr_splitter,
        # training
        "n_frozen_epochs": 0,
        "frozen_lr": None,
        "n_unfrozen_epochs": 10,
        "unfrozen_lrs": [1e-6, 2e-5],
    },
    # roberta
    "blurr_roberta_large": {
        "comp_trainer_cls": fastai.FastaiCompTrainer,
        # hf objects
        "model_checkpoint": "roberta-large",
         "hf_config_kwargs": {
            "num_labels": 6,
            "classifier_dropout ": 0.15,  # default = hidden_dropout_prob
            "hidden_dropout_prob": 0.1,  # default = 0.1
            "attention_probs_dropout_prob": 0.1,  # default = 0.1
            "layer_norm_eps": 1e-12 # default = 1e-7
        },
        "hf_tokenizer_kwargs": {},
        # data
        "anchor_col": "anchor",
        "target_col": "target",
        "max_length": 140,
        "include_labels": True,
        "tok_kwargs": {},
        "batch_size": 8,
        # learner
        "adam_beta1": 0.9,
        "adam_beta2": 0.98,
        "adam_eps": 1e-6,
        "weight_decay": 0.01,
        "max_grad_norm": 1.0,
        "save_best_model": True,
        "use_fp16": True,
        # training
        "n_frozen_epochs": 0,
        "frozen_lr": None,
        "n_unfrozen_epochs": 10,
        "unfrozen_lrs": [1e-6, 2e-5],
    },
    "blurr_roberta_large_custom_head": {
        "comp_trainer_cls": fastai.FastaiCompTrainer,
        # hf objects
        "model_checkpoint": "roberta-large",
        "hf_model_cls": DebertaV2Model,
        "hf_config_kwargs": {
            "hidden_dropout_prob": 0.0,  # default = 0.1
            "attention_probs_dropout_prob": 0.1,  # default = 0.1
            "layer_norm_eps": 1e-7 # default = 1e-7
        },
        "hf_tokenizer_kwargs": {},
        # data
        "anchor_col": "anchor",
        "target_col": "target",
        "max_length": 140,
        "include_labels": False,
        "tok_kwargs": {},
        "batch_size": 16,
        # learner
        "adam_beta1": 0.9,
        "adam_beta2": 0.999,
        "adam_eps": 1e-5,
        "weight_decay": 0.01,
        "max_grad_norm": None,
        "save_best_model": True,
        "use_fp16": True,
        "custom_base_model_cls": fastai.CustomHeadModelWrapper,
        "custom_base_model_kwargs": {"p": 0.2, "y_range": None, "layer_norm_eps": 1e-6}, # defaults => {"p": 0.0, "y_range": None, "layer_norm_eps": 1e-5}
        "splitter": fastai.custom_head_blurr_splitter,
        # training
        "n_frozen_epochs": 0,
        "frozen_lr": None,
        "n_unfrozen_epochs": 10,
        "unfrozen_lrs": [1e-6, 2e-5],
    },
    "blurr_distilroberta_base": {
        "comp_trainer_cls": fastai.FastaiCompTrainer,
        # hf objects
        "model_checkpoint": "distilroberta-base",
         "hf_config_kwargs": {
            "num_labels": 6,
            "classifier_dropout ": 0.15,  # default = hidden_dropout_prob
            "hidden_dropout_prob": 0.1,  # default = 0.1
            "attention_probs_dropout_prob": 0.1,  # default = 0.1
            "layer_norm_eps": 1e-12 # default = 1e-7
        },
        "hf_tokenizer_kwargs": {},
        # data
        "anchor_col": "anchor",
        "target_col": "target",
        "max_length": 140,
        "include_labels": True,
        "tok_kwargs": {},
        "batch_size": 8,
        # learner
        "adam_beta1": 0.9,
        "adam_beta2": 0.98,
        "adam_eps": 1e-6,
        "weight_decay": 0.01,
        "max_grad_norm": 1.0,
        "save_best_model": True,
        "use_fp16": True,
        # training
        "n_frozen_epochs": 0,
        "frozen_lr": None,
        "n_unfrozen_epochs": 10,
        "unfrozen_lrs": [1e-6, 2e-5],
    },
    # anferico/bert-for-patents
    "blurr_bert_patents_large": {
        "comp_trainer_cls": fastai.FastaiCompTrainer,
        # hf objects
        "model_checkpoint": "anferico/bert-for-patents",
         "hf_config_kwargs": {
            "num_labels": 6,
            "classifier_dropout ": 0.15,  # default = hidden_dropout_prob
            "hidden_dropout_prob": 0.1,  # default = 0.1
            "attention_probs_dropout_prob": 0.1,  # default = 0.1
            "layer_norm_eps": 1e-12 # default = 1e-7
        },
        "hf_tokenizer_kwargs": {},
        # data
        "anchor_col": "anchor",
        "target_col": "target",
        "max_length": 140,
        "include_labels": True,
        "tok_kwargs": {},
        "batch_size": 8,
        # learner
        "adam_beta1": 0.9,
        "adam_beta2": 0.999,
        "adam_eps": 1e-6,
        "weight_decay": 0.01,
        "max_grad_norm": 1.0,
        "save_best_model": True,
        "use_fp16": True,
        # training
        "n_frozen_epochs": 0,
        "frozen_lr": None,
        "n_unfrozen_epochs": 10,
        "unfrozen_lrs": [1e-6, 2e-5],
    },
    "blurr_bert_patents_large_custom_head": {
        "comp_trainer_cls": fastai.FastaiCompTrainer,
        # hf objects
        "model_checkpoint": "anferico/bert-for-patents",
        "hf_model_cls": DebertaV2Model,
        "hf_config_kwargs": {
            "hidden_dropout_prob": 0.0,  # default = 0.1
            "attention_probs_dropout_prob": 0.1,  # default = 0.1
            "layer_norm_eps": 1e-7 # default = 1e-7
        },
        "hf_tokenizer_kwargs": {},
        # data
        "anchor_col": "anchor",
        "target_col": "target",
        "max_length": 140,
        "include_labels": False,
        "tok_kwargs": {},
        "batch_size": 16,
        # learner
        "adam_beta1": 0.9,
        "adam_beta2": 0.999,
        "adam_eps": 1e-5,
        "weight_decay": 0.01,
        "max_grad_norm": None,
        "save_best_model": True,
        "use_fp16": True,
        "custom_base_model_cls": fastai.CustomHeadModelWrapper,
        "custom_base_model_kwargs": {"p": 0.2, "y_range": None, "layer_norm_eps": 1e-6}, # defaults => {"p": 0.0, "y_range": None, "layer_norm_eps": 1e-5}
        "splitter": fastai.custom_head_blurr_splitter,
        # training
        "n_frozen_epochs": 0,
        "frozen_lr": None,
        "n_unfrozen_epochs": 10,
        "unfrozen_lrs": [1e-6, 2e-5],
    },
    # bart
    "bart_base": {
        "comp_trainer_cls": fastai.FastaiCompTrainer,
        # hf objects
        "model_checkpoint": "facebook/bart-base",
        "hf_config_kwargs": {
            "num_labels": 6,
            "classifier_dropout": 0.15,
            "dropout": 0.1,
            "attention_dropout": 0.0,
            "activation_dropout": 0.0,
            
        },
        "hf_tokenizer_kwargs": {},
        # data
        "anchor_col": "anchor",
        "target_col": "target",
        "max_length": 140,
        "include_labels": True,
        "tok_kwargs": {},
        "batch_size": 64,
        # learner
        "adam_beta1": 0.9,
        "adam_beta2": 0.999,
        "adam_eps": 1e-7,
        "weight_decay": 0.01,
        "max_grad_norm": 1.0,
        "save_best_model": True,
        "use_fp16": True,
        # training
        "n_frozen_epochs": 1,
        "frozen_lr": 5e-3,
        "n_unfrozen_epochs": 10,
        "unfrozen_lrs": [1e-6, 5e-4],
    },
    "bart_large": {
        "comp_trainer_cls": fastai.FastaiCompTrainer,
        # hf objects
        "model_checkpoint": "facebook/bart-large",
        "hf_config_kwargs": {
            "num_labels": 6,
             "classifier_dropout": 0.15,
            "dropout": 0.1,
            "attention_dropout": 0.0,
            "activation_dropout": 0.0,
        },
        "hf_tokenizer_kwargs": {},
        # data
        "anchor_col": "anchor",
        "target_col": "target",
        "max_length": 140,
        "include_labels": True,
        "tok_kwargs": {},
        "batch_size": 8,
        # learner
        "adam_beta1": 0.9,
        "adam_beta2": 0.999,
        "adam_eps": 1e-7,
        "weight_decay": 0.01,
        "max_grad_norm": 1.0,
        "save_best_model": True,
        "use_fp16": True,
        # training
        "n_frozen_epochs": 1,
        "frozen_lr": 5e-3,
        "n_unfrozen_epochs": 10,
        "unfrozen_lrs": [1e-6, 3e-5],
    },
    # xlnet
    "xlnet_base_cased": {
        "comp_trainer_cls": fastai.FastaiCompTrainer,
        # hf objects
        "model_checkpoint": "xlnet-base-cased",
        "hf_config_kwargs": {
            "num_labels": 6,
            "dropout": 0.1,
            "summary_last_dropout": 0.1,
        },
        "hf_tokenizer_kwargs": {},
        # data
        "anchor_col": "anchor",
        "target_col": "target",
        "max_length": 140,
        "include_labels": True,
        "tok_kwargs": {},
        "batch_size": 64,
        # learner
        "adam_beta1": 0.9,
        "adam_beta2": 0.999,
        "adam_eps": 1e-6,
        "weight_decay": 0.01,
        "max_grad_norm": 1.0,
        "save_best_model": True,
        "use_fp16": True,
        # training
        "n_frozen_epochs": 0,
        "frozen_lr": None,
        "n_unfrozen_epochs": 10,
        "unfrozen_lrs": [1e-5, 1e-2],
    },
    "xlnet_base_cased_custom_head": {
        "comp_trainer_cls": fastai.FastaiCompTrainer,
        # hf objects
        "model_checkpoint": "xlnet-base-cased",
        "hf_model_cls": DebertaV2Model,
        "hf_config_kwargs": {
            "hidden_dropout_prob": 0.0,  # default = 0.1
            "attention_probs_dropout_prob": 0.1,  # default = 0.1
            "layer_norm_eps": 1e-7 # default = 1e-7
        },
        "hf_tokenizer_kwargs": {},
        # data
        "anchor_col": "anchor",
        "target_col": "target",
        "max_length": 140,
        "include_labels": False,
        "tok_kwargs": {},
        "batch_size": 64,
        # learner
        "adam_beta1": 0.9,
        "adam_beta2": 0.999,
        "adam_eps": 1e-5,
        "weight_decay": 0.01,
        "max_grad_norm": None,
        "save_best_model": True,
        "use_fp16": True,
        "custom_base_model_cls": fastai.CustomHeadModelWrapper,
        "custom_base_model_kwargs": {"p": 0.2, "y_range": None, "layer_norm_eps": 1e-6}, # defaults => {"p": 0.0, "y_range": None, "layer_norm_eps": 1e-5}
        "splitter": fastai.custom_head_blurr_splitter,
        # training
        "n_frozen_epochs": 0,
        "frozen_lr": None,
        "n_unfrozen_epochs": 10,
        "unfrozen_lrs": [1e-5, 1e-2],
    },
    "xlnet_large_cased": {
        "comp_trainer_cls": fastai.FastaiCompTrainer,
        # hf objects
        "model_checkpoint": "xlnet-large-cased",
        "hf_config_kwargs": {
            "num_labels": 6,
            "dropout": 0.1,
            "summary_last_dropout": 0.1,
        },
        "hf_tokenizer_kwargs": {},
        # data
        "anchor_col": "anchor",
        "target_col": "target",
        "max_length": 140,
        "include_labels": True,
        "tok_kwargs": {},
        "batch_size": 16,
        # learner
        "adam_beta1": 0.9,
        "adam_beta2": 0.999,
        "adam_eps": 1e-6,
        "weight_decay": 0.01,
        "max_grad_norm": 1.0,
        "save_best_model": True,
        "use_fp16": True,
        # training
        "n_frozen_epochs": 0,
        "frozen_lr": None,
        "n_unfrozen_epochs": 10,
        "unfrozen_lrs": [1e-5, 3e-4],
    },
    "xlnet_large_cased_custom_head": {
        "comp_trainer_cls": fastai.FastaiCompTrainer,
        # hf objects
        "model_checkpoint": "xlnet-large-cased",
        "hf_model_cls": DebertaV2Model,
        "hf_config_kwargs": {
            "hidden_dropout_prob": 0.0,  # default = 0.1
            "attention_probs_dropout_prob": 0.1,  # default = 0.1
            "layer_norm_eps": 1e-7 # default = 1e-7
        },
        "hf_tokenizer_kwargs": {},
        # data
        "anchor_col": "anchor",
        "target_col": "target",
        "max_length": 140,
        "include_labels": False,
        "tok_kwargs": {},
        "batch_size": 16,
        # learner
        "adam_beta1": 0.9,
        "adam_beta2": 0.999,
        "adam_eps": 1e-5,
        "weight_decay": 0.01,
        "max_grad_norm": None,
        "save_best_model": True,
        "use_fp16": True,
        "custom_base_model_cls": fastai.CustomHeadModelWrapper,
        "custom_base_model_kwargs": {"p": 0.2, "y_range": None, "layer_norm_eps": 1e-6}, # defaults => {"p": 0.0, "y_range": None, "layer_norm_eps": 1e-5}
        "splitter": fastai.custom_head_blurr_splitter,
        # training
        "n_frozen_epochs": 0,
        "frozen_lr": None,
        "n_unfrozen_epochs": 10,
        "unfrozen_lrs": [1e-5, 3e-4],
    },
    # electra
    "blurr_electra_small_discriminator": {
        "comp_trainer_cls": fastai.FastaiCompTrainer,
        # hf objects
        "model_checkpoint": "google/electra-small-discriminator",
        "hf_config_kwargs": {
            "num_labels": 6,
            "classifier_dropout": 0.15,  # default = None
            "summary_last_dropout": 0.0,  # default = 0.0
            "hidden_dropout_prob": 0.1,  # default = 0.1
            "attention_probs_dropout_prob": 0.1,  # default = 0.1
            "summary_type": "first" #default = "first"
        },
        "hf_tokenizer_kwargs": {},
        # data
        "anchor_col": "anchor",
        "target_col": "target",
        "max_length": 140,
        "include_labels": True,
        "tok_kwargs": {},
        "batch_size": 128,
        # learner
        "adam_beta1": 0.9,
        "adam_beta2": 0.999,
        "adam_eps": 1e-6,
        "weight_decay": 0.01,
        "max_grad_norm": 1.0,
        "save_best_model": True,
        "use_fp16": True,
        # training
        "n_frozen_epochs": 0,
        "frozen_lr": None,
        "n_unfrozen_epochs": 10,
        "unfrozen_lrs": [3e-6, 3e-3],
    },
    "blurr_electra_large_discriminator": {
        "comp_trainer_cls": fastai.FastaiCompTrainer,
        # hf objects
        "model_checkpoint": "google/electra-large-discriminator",
        "hf_config_kwargs": {
            "num_labels": 6,
            "classifier_dropout": 0.15,  # default = None
            "summary_last_dropout": 0.0,  # default = 0.0
            "hidden_dropout_prob": 0.1,  # default = 0.1
            "attention_probs_dropout_prob": 0.1,  # default = 0.1
            # "summary_type": "last" #default = "first"
        },
        "hf_tokenizer_kwargs": {},
        # data
        "anchor_col": "anchor",
        "target_col": "target",
        "max_length": 140,
        "include_labels": True,
        "tok_kwargs": {},
        "batch_size": 16,
        # learner
        "adam_beta1": 0.9,
        "adam_beta2": 0.999,
        "adam_eps": 1e-6,
        "weight_decay": 0.01,
        "max_grad_norm": 1.0,
        "save_best_model": True,
        "use_fp16": True,
        # training
        "n_frozen_epochs": 0,
        "frozen_lr": None,
        "n_unfrozen_epochs": 10,
        "unfrozen_lrs": [3e-6, 3e-3],
    },
    # # coco-lm
    # "blurr_cocolm_large": {
    #     "comp_trainer_cls": fastai_cocolm.FastaiCocoLmCompTrainer,
    #     #hf objects
    #     "model_checkpoint": "microsoft/cocolm-large",
    #      "hf_config_kwargs": {
    #         "num_labels": 6,
    #         "hidden_dropout_prob": 0.1, # default = 0.1
    #         "attention_probs_dropout_prob": 0.1, # default = 0.1
    #         "summary_type": "first", # default = "first"
    #         "summary_last_dropout": 0.1, # default = 0.1
    #     },
    #     "hf_tokenizer_kwargs": {},
    #     # data
    #     "anchor_col": "anchor",
    #     "target_col": "target",
    #     "max_length": 140,
    #     "include_labels": True,
    #     "tok_kwargs": {},
    #     "batch_size": 128,
    #     # learner
    #     "adam_beta1": 0.9,
    #     "adam_beta2": 0.999,
    #     "adam_eps": 1e-5,
    #     "weight_decay": 0.01,
    #     "max_grad_norm": 1.0,
    #     "save_best_model": True,
    #     "use_fp16": True,
    #     # training
    #     "n_frozen_epochs": 0,
    #     "frozen_lr": None,
    #     "n_unfrozen_epochs": 10,
    #     "unfrozen_lrs": [3e-6, 5e-5],
    # },
    # "blurr_cocolm_large_custom_head": {
    #     "comp_trainer_cls": fastai_cocolm.FastaiCocoLmCompTrainer,
    #     # hf objects
    #     "model_checkpoint": "microsoft/cocolm-large",
    #     "hf_model_cls": DebertaV2Model,
    #     "hf_config_kwargs": {
    #         "hidden_dropout_prob": 0.0,  # default = 0.1
    #         "attention_probs_dropout_prob": 0.1,  # default = 0.1
    #         "layer_norm_eps": 1e-7 # default = 1e-7
    #     },
    #     "hf_tokenizer_kwargs": {},
    #     # data
    #     "anchor_col": "anchor",
    #     "target_col": "target",
    #     "max_length": 140,
    #     "include_labels": False,
    #     "tok_kwargs": {},
    #     "batch_size": 128,
    #     # learner
    #     "adam_beta1": 0.9,
    #     "adam_beta2": 0.999,
    #     "adam_eps": 1e-5,
    #     "weight_decay": 0.01,
    #     "max_grad_norm": None,
    #     "save_best_model": True,
    #     "use_fp16": True,
    #     "custom_base_model_cls": fastai.CustomHeadModelWrapper,
    #     "custom_base_model_kwargs": {"p": 0.2, "y_range": None, "layer_norm_eps": 1e-6}, # defaults => {"p": 0.0, "y_range": None, "layer_norm_eps": 1e-5}
    #     "splitter": fastai.custom_head_blurr_splitter,
    #     # training
    #     "n_frozen_epochs": 0,
    #     "frozen_lr": None,
    #     "n_unfrozen_epochs": 10,
    #     "unfrozen_lrs": [3e-6, 5e-5],
    # },
}

