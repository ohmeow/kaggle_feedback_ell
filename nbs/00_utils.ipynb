{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |default_exp utils\n",
    "# |default_cls_lvl 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils\n",
    "\n",
    "> Competition utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "from __future__ import annotations\n",
    "\n",
    "import abc, datetime, random, os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from fastai.losses import CrossEntropyLossFlat\n",
    "from fastai.test_utils import show_install\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from fastai.metrics import rmse, AccumMetric\n",
    "from fastcore.test import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import pdb\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "# from fastcore.test import *\n",
    "from nbdev.showdoc import show_doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU #0: GeForce GTX 1660 Ti with Max-Q Design\n"
     ]
    }
   ],
   "source": [
    "# |hide\n",
    "# |cuda\n",
    "gpu_num = int(os.getenv(\"USE_GPU\", 0))\n",
    "\n",
    "torch.cuda.set_device(gpu_num)\n",
    "print(f\"Using GPU #{torch.cuda.current_device()}: {torch.cuda.get_device_name()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defaults\n",
    "\n",
    "Application wide defaults go here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "default_seed = int(os.getenv(\"RANDOM_SEED\", 42))\n",
    "kaggle_comp = os.getenv(\"KAGGLE_COMP\", \"feedback-prize-english-language-learning\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development environment\n",
    "\n",
    "Information about where your code is running and your compute capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def detect_env():\n",
    "    \"\"\"A helper function that detects where you are running code\"\"\"\n",
    "    if os.environ.get(\"KAGGLE_KERNEL_RUN_TYPE\", False):\n",
    "        run_env = \"kaggle\"\n",
    "    elif os.path.isdir(\"/content\"):\n",
    "        run_env = \"colab\"\n",
    "    elif os.path.isdir(\"../nbs\") or os.path.isdir(\"../../nbs\"):\n",
    "        run_env = \"local_nb\"\n",
    "    else:\n",
    "        run_env = \"script\"\n",
    "\n",
    "    return run_env\n",
    "\n",
    "\n",
    "run_env = detect_env()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local_nb\n"
     ]
    }
   ],
   "source": [
    "# |eval: false\n",
    "print(run_env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def print_dev_environment():\n",
    "    \"\"\"Provides details on your development environment including packages installed, cuda/cudnn availability, GPUs, etc.\"\"\"\n",
    "    print(show_install())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "```text\n",
      "=== Software === \n",
      "python        : 3.9.13\n",
      "fastai        : 2.7.9\n",
      "fastcore      : 1.5.26\n",
      "fastprogress  : 1.0.3\n",
      "torch         : 1.12.1+cu102\n",
      "nvidia driver : 455.32\n",
      "torch cuda    : 10.2 / is available\n",
      "torch cudnn   : 7605 / is enabled\n",
      "\n",
      "=== Hardware === \n",
      "nvidia gpus   : 1\n",
      "torch devices : 1\n",
      "  - gpu0      : GeForce GTX 1660 Ti with Max-Q Design\n",
      "\n",
      "=== Environment === \n",
      "platform      : Linux-5.4.0-42-generic-x86_64-with-glibc2.31\n",
      "distro        : #46-Ubuntu SMP Fri Jul 10 00:24:02 UTC 2020\n",
      "conda env     : kaggle_feedback_ell\n",
      "python        : /home/mhenze/miniconda3/envs/kaggle_feedback_ell/bin/python\n",
      "sys.path      : /home/mhenze/kaggle_feedback_ell/nbs\n",
      "/home/mhenze/miniconda3/envs/kaggle_feedback_ell/lib/python39.zip\n",
      "/home/mhenze/miniconda3/envs/kaggle_feedback_ell/lib/python3.9\n",
      "/home/mhenze/miniconda3/envs/kaggle_feedback_ell/lib/python3.9/lib-dynload\n",
      "\n",
      "/home/mhenze/miniconda3/envs/kaggle_feedback_ell/lib/python3.9/site-packages\n",
      "```\n",
      "\n",
      "Please make sure to include opening/closing ``` when you paste into forums/github to make the reports appear formatted as code sections.\n",
      "\n",
      "Optional package(s) to enhance the diagnostics can be installed with:\n",
      "pip install distro\n",
      "Once installed, re-run this utility to get the additional information\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# |output: false\n",
    "print_dev_environment()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Competition Setup\n",
    "\n",
    "**NOTE**: The first thing you should run when setting things up after you've deinfed your `kaggle_comp`, is `setup_comp()`. This method will ensure all the necessary folders are created as well as download the competition data if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def get_paths(override_project_root=None):\n",
    "    \"\"\"Returns data, models, and log folder paths based on your where you are running the code\"\"\"\n",
    "    if run_env == \"kaggle\":\n",
    "        data_path = Path(\".\")\n",
    "        comp_data_path = clean_data_path = Path(f\"../input/{kaggle_comp}\")\n",
    "        working_path = Path(\"/kaggle/working\")\n",
    "        models_path = working_path / \"models\"\n",
    "        logs_path = working_path / \"logs\"\n",
    "\n",
    "    elif run_env == \"colab\":\n",
    "        proj_root_path = override_project_root or Path(\".\")\n",
    "\n",
    "        data_path = proj_root_path\n",
    "        comp_data_path = clean_data_path = data_path\n",
    "        models_path = data_path / \"models\"\n",
    "        logs_path = data_path / \"logs\"\n",
    "\n",
    "    elif run_env == \"local_nb\":\n",
    "        proj_root_path = override_project_root or Path(\"..\")\n",
    "\n",
    "        data_path = Path(proj_root_path / \"data\")\n",
    "        comp_data_path = data_path / \"comp\"\n",
    "        clean_data_path = data_path / \"clean\"\n",
    "        models_path = Path(proj_root_path / \"models\")\n",
    "        logs_path = Path(proj_root_path / \"logs\")\n",
    "\n",
    "        comp_data_path.mkdir(parents=True, exist_ok=True)\n",
    "        clean_data_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    elif run_env == \"script\":\n",
    "        proj_root_path = override_project_root or Path(\".\")\n",
    "\n",
    "        data_path = Path(proj_root_path / \"data\")\n",
    "        comp_data_path = data_path / \"comp\"\n",
    "        clean_data_path = data_path / \"clean\"\n",
    "        models_path = Path(proj_root_path / \"models\")\n",
    "        logs_path = Path(proj_root_path / \"logs\")\n",
    "\n",
    "        comp_data_path.mkdir(parents=True, exist_ok=True)\n",
    "        clean_data_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        models_path.mkdir(parents=True, exist_ok=True)\n",
    "        logs_path.mkdir(parents=True, exist_ok=True)\n",
    "    except:\n",
    "        print(\"Unable to create models and logs folders\")\n",
    "\n",
    "    return data_path, comp_data_path, clean_data_path, models_path, logs_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def setup_comp(override_project_root=None, comp_data_path_override=None):\n",
    "    \"\"\"Ensures that the expected data, models, and logs folders exist and that the competition data exists in the 'comp_data_path'.\"\"\"\n",
    "\n",
    "    if comp_data_path_override is not None:\n",
    "        comp_data_path = comp_data_path_override\n",
    "    else:\n",
    "        _, comp_data_path, *_ = get_paths(override_project_root)\n",
    "\n",
    "    if run_env != \"kaggle\":\n",
    "        from kaggle import api\n",
    "        \n",
    "        if not comp_data_path.exists() or not any(comp_data_path.iterdir()):\n",
    "            import zipfile\n",
    "\n",
    "            api.competition_download_cli(kaggle_comp)\n",
    "\n",
    "            zipfile.ZipFile(f\"{kaggle_comp}.zip\").extractall(comp_data_path)\n",
    "            Path(f\"{kaggle_comp}.zip\").unlink(missing_ok=True)\n",
    "\n",
    "        return comp_data_path\n",
    "    else:\n",
    "        return Path(f\"../input/{kaggle_comp}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('../data/comp')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# |eval: false\n",
    "setup_comp()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Competition Metrics\n",
    "\n",
    "This competition is evaluated using \"columnwise root mean squared error\", i.e. the rmse per target column averaged over all targets. From the competition website:\n",
    "\n",
    "> For each text_id in the test set, you must predict a value for each of the six analytic measures "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def comp_metric_score(preds, targs):\n",
    "    \"\"\"This competition is evaluated using \"columnwise root mean squared error\". Expects numpy arrays.\"\"\"\n",
    "    len_target_cols = targs.shape[1]\n",
    "    score = [0] * len_target_cols\n",
    "    for i in range(len_target_cols):\n",
    "        score[i] = np.sqrt(mean_squared_error(preds[:, i], targs[:, i]))\n",
    "    return np.mean(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def MCRMSE(dim_argmax=None, **kwargs):\n",
    "    \"columnwise root mean squared error for regression problem\"\n",
    "    def mcrmse(x,y): return comp_metric_score(x.cpu().numpy(),y.cpu().numpy())\n",
    "    return AccumMetric(mcrmse, invert_arg=False, flatten=False, dim_argmax=dim_argmax, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify our comp_metric_score calculates the metric correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [4, 5, 6]])\n",
      "tensor([[0.5000, 2.0000, 2.5000],\n",
      "        [3.5000, 5.2000, 5.8000]])\n"
     ]
    }
   ],
   "source": [
    "preds = torch.tensor([[0, 1, 2], [4, 5, 6]]) #.transpose(0, 1)\n",
    "targets = torch.tensor([[0.5, 2, 2.5], [3.5, 5.2, 5.8]]) #.transpose(0, 1)\n",
    "\n",
    "print(preds)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5339663028717041 0.5339662779488101\n"
     ]
    }
   ],
   "source": [
    "check = ((rmse(preds[:, 0], targets[:, 0]) + rmse(preds[:, 1], targets[:, 1]) + rmse(preds[:, 2], targets[:, 2]))/3).item()\n",
    "score = comp_metric_score(preds.numpy(), targets.numpy())\n",
    "\n",
    "print(check, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(score, check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5339662779488101"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_met = MCRMSE()\n",
    "test_met(preds=preds, targs=targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(test_met(preds=preds, targs=targets), check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RMSELoss_0(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, yhat, y):\n",
    "        loss = torch.sqrt(self.mse(yhat, y) + self.eps)\n",
    "        return loss\n",
    "\n",
    "\n",
    "class MCRMSELoss(nn.Module):\n",
    "    def __init__(self, num_scored=6):\n",
    "        super().__init__()\n",
    "        self.rmse = RMSELoss_0()\n",
    "        self.num_scored = num_scored\n",
    "\n",
    "    def forward(self, yhat, y):\n",
    "        score = 0\n",
    "        for i in range(self.num_scored):\n",
    "            score += self.rmse(yhat[:, i], y[:, i]) / self.num_scored\n",
    "\n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def rev_phrase(foo: str):\n",
    "    return \" \".join(foo.split()[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'phrase test'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev_phrase(\"test phrase\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Competition Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CompTrainer(abc.ABC):\n",
    "    def __init__(self, train_config, model_name, model_output_path=\"models\", log_output_path=\"logs\", **kwargs):\n",
    "        self.train_config = train_config\n",
    "        self.model_name = model_name\n",
    "        self.model_output_path = Path(model_output_path)\n",
    "        self.log_output_path = Path(log_output_path)\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def train(self, CFG, data, experiment_name=None , n_fold=5, run_id=-1, grid_id=-1, seed=None, verbose: bool = True):\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def predict(self, model_name, data):\n",
    "        pass\n",
    "\n",
    "    def get_value_for(self, attr, CFG, default):\n",
    "        val = getattr(CFG, attr, None)\n",
    "        return val if val is not None else self.train_config.get(attr, default)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def get_run_id():\n",
    "    run_id = str(datetime.datetime.now())[:16].replace(\":\", \"_\").replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "    return run_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022_11_10_22_03\n"
     ]
    }
   ],
   "source": [
    "print(get_run_id())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('kaggle_feedback_ell')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
