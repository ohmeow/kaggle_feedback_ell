{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from fastai.callback.all import *\n",
    "from fastai.data.block import CategoryBlock, ColReader, ColSplitter, DataBlock, IndexSplitter, RegressionBlock\n",
    "from fastai.imports import *\n",
    "from fastai.learner import *\n",
    "from fastai.optimizer import Adam\n",
    "from fastai.metrics import *\n",
    "from fastai.torch_core import *\n",
    "from fastai.torch_imports import *\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, logging\n",
    "\n",
    "from blurr.callbacks import GradientCheckpointing\n",
    "from blurr.text.data.core import TextBlock\n",
    "from blurr.text.modeling.core import BaseModelWrapper, BaseModelCallback, blurr_splitter\n",
    "from blurr.text.utils import get_hf_objects\n",
    "from blurr.utils import PreCalculatedMSELoss, print_versions, set_seed\n",
    "\n",
    "from kaggle_comp import utils\n",
    "\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from fastai.metrics import rmse, AccumMetric\n",
    "\n",
    "from fastai.losses import MSELossFlat, L1LossFlat\n",
    "\n",
    "from torch.nn import MSELoss\n",
    "from torch import nn\n",
    "\n",
    "# silence all the HF warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "logging.set_verbosity_error()\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('4.22.0', '2.7.9', '1.0.5')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "import fastai\n",
    "import blurr\n",
    "\n",
    "transformers.__version__, fastai.__version__, blurr.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup/Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU #0: GeForce GTX 1660 Ti with Max-Q Design\n"
     ]
    }
   ],
   "source": [
    "device_num = 0\n",
    "seed = 42 or utils.default_seed\n",
    "\n",
    "torch.cuda.set_device(device_num)\n",
    "print(f\"Using GPU #{torch.cuda.current_device()}: {torch.cuda.get_device_name()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_path = Path(\"../../data/comp\")\n",
    "clean_data_path = Path(\"../../data/clean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Competition Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of rows in TRAIN: 3911\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>full_text</th>\n",
       "      <th>cohesion</th>\n",
       "      <th>syntax</th>\n",
       "      <th>vocabulary</th>\n",
       "      <th>phraseology</th>\n",
       "      <th>grammar</th>\n",
       "      <th>conventions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0016926B079C</td>\n",
       "      <td>I think that students would benefit from learning at home,because they wont have to change and get up early in the morning to shower and do there hair. taking only classes helps them because at there house they'll be pay more attention. they will be comfortable at home.\\n\\nThe hardest part of school is getting ready. you wake up go brush your teeth and go to your closet and look at your cloths. after you think you picked a outfit u go look in the mirror and youll either not like it or you look and see a stain. Then you'll have to change. with the online classes you can wear anything and st...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0022683E9EA5</td>\n",
       "      <td>When a problem is a change you have to let it do the best on you no matter what is happening it can change your mind. sometimes you need to wake up and look what is around you because problems are the best way to change what you want to change along time ago. A\\n\\nproblem is a change for you because it can make you see different and help you to understand how tings wok.\\n\\nFirst of all it can make you see different then the others. For example i remember that when i came to the United States i think that nothing was going to change me because i think that nothing was going to change me bec...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00299B378633</td>\n",
       "      <td>Dear, Principal\\n\\nIf u change the school policy of having a grade b average that unfair. Because many students have a C average. So that means that they cant go out for sports or other activities they want to do bad. That's like taking everything they have. What if kids want to become good at something, but now they cant because of that school policy. If they have a C average they should still be able to go out for sports or activities. A C average isn't that bad, its higher then a D average. If the school police was if you have a D average of lower they shouldn't do sports or activities....</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>003885A45F42</td>\n",
       "      <td>The best time in life is when you become yourself. I agree that the greatest accomplishment, is when you be yourself in a world that constantly trying to make you something else. Because you make your own choices, you become more happy, and you respect others.\\n\\nFirst, you make your own choices by being yourself. Becoming yourself means that you should be able to make your own choices and not be shy or afraid of what you're doing. Because you're defining yourself by doing those things that you want. Some people follow others, therefore, they don't make their own choices. People are afraid...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0049B1DF5CCC</td>\n",
       "      <td>Small act of kindness can impact in other people can change people to become better persons you can have an impact of kindess with a homeles that can change his life or with some who needed they are going to know you are a nice person if you are a nice person everywhere you go people is going to like your personality so you have to be a nice person with others like a old women triying to cross the road thats a impact of kindness when you do that you feel a greate person you can change people in the way they think by helping others treating nice other people give them some advice when you s...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text_id  \\\n",
       "0  0016926B079C   \n",
       "1  0022683E9EA5   \n",
       "2  00299B378633   \n",
       "3  003885A45F42   \n",
       "4  0049B1DF5CCC   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 full_text  \\\n",
       "0  I think that students would benefit from learning at home,because they wont have to change and get up early in the morning to shower and do there hair. taking only classes helps them because at there house they'll be pay more attention. they will be comfortable at home.\\n\\nThe hardest part of school is getting ready. you wake up go brush your teeth and go to your closet and look at your cloths. after you think you picked a outfit u go look in the mirror and youll either not like it or you look and see a stain. Then you'll have to change. with the online classes you can wear anything and st...   \n",
       "1  When a problem is a change you have to let it do the best on you no matter what is happening it can change your mind. sometimes you need to wake up and look what is around you because problems are the best way to change what you want to change along time ago. A\\n\\nproblem is a change for you because it can make you see different and help you to understand how tings wok.\\n\\nFirst of all it can make you see different then the others. For example i remember that when i came to the United States i think that nothing was going to change me because i think that nothing was going to change me bec...   \n",
       "2  Dear, Principal\\n\\nIf u change the school policy of having a grade b average that unfair. Because many students have a C average. So that means that they cant go out for sports or other activities they want to do bad. That's like taking everything they have. What if kids want to become good at something, but now they cant because of that school policy. If they have a C average they should still be able to go out for sports or activities. A C average isn't that bad, its higher then a D average. If the school police was if you have a D average of lower they shouldn't do sports or activities....   \n",
       "3  The best time in life is when you become yourself. I agree that the greatest accomplishment, is when you be yourself in a world that constantly trying to make you something else. Because you make your own choices, you become more happy, and you respect others.\\n\\nFirst, you make your own choices by being yourself. Becoming yourself means that you should be able to make your own choices and not be shy or afraid of what you're doing. Because you're defining yourself by doing those things that you want. Some people follow others, therefore, they don't make their own choices. People are afraid...   \n",
       "4  Small act of kindness can impact in other people can change people to become better persons you can have an impact of kindess with a homeles that can change his life or with some who needed they are going to know you are a nice person if you are a nice person everywhere you go people is going to like your personality so you have to be a nice person with others like a old women triying to cross the road thats a impact of kindness when you do that you feel a greate person you can change people in the way they think by helping others treating nice other people give them some advice when you s...   \n",
       "\n",
       "   cohesion  syntax  vocabulary  phraseology  grammar  conventions  \n",
       "0       3.5     3.5         3.0          3.0      4.0          3.0  \n",
       "1       2.5     2.5         3.0          2.0      2.0          2.5  \n",
       "2       3.0     3.5         3.0          3.0      3.0          2.5  \n",
       "3       4.5     4.5         4.5          4.5      4.0          5.0  \n",
       "4       2.5     3.0         3.0          3.0      2.5          2.5  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is already the train data with folds:\n",
    "# train_df = pd.read_csv(clean_data_path / \"train_folds.csv\")\n",
    "\n",
    "# the raw training data, for processing from scratch:\n",
    "train_df = pd.read_csv(raw_data_path / \"train.csv\")\n",
    "\n",
    "\n",
    "print(f\"# of rows in TRAIN: {len(train_df)}\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of rows in TEST: 3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>full_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000C359D63E</td>\n",
       "      <td>when a person has no experience on a job their is always going to be good people to help you and try to explane the job you need to get done in life you were not born with knowing everything. Life is bassicly about learing new things every single day even though without experience because life is simple and we must live happy and around with the people we love. When a person thinks they know everything in life they dont do good because they trying to make the other person less then others you must be kind to those the dont have experience because you may not know some day you will go to a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000BAD50D026</td>\n",
       "      <td>Do you think students would benefit from being able attend classes from home?\\n\\nYes! its benefit for student who attend classes from home. Because some student want to attend classes from home because they thinks it's very important for them . And they think they can learned fast, and understand than they student who attend classes from school. For example my friend told me that she's attand classes from home it's good for her, because they is some subject she didn't understand when she attend classes from school but when she attend the home classes she good for that subject. she like sci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00367BB2546B</td>\n",
       "      <td>Thomas Jefferson once states that \"it is wonderful how much we can get done when we are always doing something,I agree to this statement. Some people say that it is always better to always be doing something than to be inactive and so nothing at all to chase after on your dreams. always doing something lets you feel a sense of accomplishment,it makes you want to strive to do even more than what you did,you can also learn something along the way.\\n\\nThe first reason that always doing is better than doing nothing is that it gives you a sense of you accomplishing something. when you finish a ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text_id  \\\n",
       "0  0000C359D63E   \n",
       "1  000BAD50D026   \n",
       "2  00367BB2546B   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 full_text  \n",
       "0  when a person has no experience on a job their is always going to be good people to help you and try to explane the job you need to get done in life you were not born with knowing everything. Life is bassicly about learing new things every single day even though without experience because life is simple and we must live happy and around with the people we love. When a person thinks they know everything in life they dont do good because they trying to make the other person less then others you must be kind to those the dont have experience because you may not know some day you will go to a ...  \n",
       "1  Do you think students would benefit from being able attend classes from home?\\n\\nYes! its benefit for student who attend classes from home. Because some student want to attend classes from home because they thinks it's very important for them . And they think they can learned fast, and understand than they student who attend classes from school. For example my friend told me that she's attand classes from home it's good for her, because they is some subject she didn't understand when she attend classes from school but when she attend the home classes she good for that subject. she like sci...  \n",
       "2  Thomas Jefferson once states that \"it is wonderful how much we can get done when we are always doing something,I agree to this statement. Some people say that it is always better to always be doing something than to be inactive and so nothing at all to chase after on your dreams. always doing something lets you feel a sense of accomplishment,it makes you want to strive to do even more than what you did,you can also learn something along the way.\\n\\nThe first reason that always doing is better than doing nothing is that it gives you a sense of you accomplishing something. when you finish a ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(raw_data_path / \"test.csv\")\n",
    "\n",
    "print(f\"# of rows in TEST: {len(test_df)}\")\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Get HF objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"microsoft/deberta-v3-small\"\n",
    "#model_checkpoint = \"distilroberta-base\"\n",
    "#batch_size = 128\n",
    "batch_size = 4\n",
    "weight_decay = 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_hf_objects(\n",
    "    model_checkpoint, model_cls=AutoModelForSequenceClassification, config_kwargs={\"num_labels\": 6}\n",
    ")\n",
    "hf_model.config.problem_type = \"multi_label_regression\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DebertaV2ForSequenceClassification(\n",
       "  (deberta): DebertaV2Model(\n",
       "    (embeddings): DebertaV2Embeddings(\n",
       "      (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): StableDropout()\n",
       "    )\n",
       "    (encoder): DebertaV2Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "        (1): DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "        (2): DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "        (3): DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "        (4): DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "        (5): DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rel_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (pooler): ContextPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): StableDropout()\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       "  (dropout): StableDropout()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Build your `DataLoaders`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2a: Define a good validation set\n",
    "\n",
    "Using 5-fold `MultilabelStratifiedKFold` already here, since we'll use that later anyway, and pick fold 0 as the validation fold. We stratify by the 6 target columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_cols = [x for x in train_df.columns if x not in ['text_id', 'full_text', 'k_fold']]\n",
    "target_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=4321)\n",
    "\n",
    "for fold, (_, val_index) in enumerate(skf.split(X = train_df, y = train_df[target_cols])):\n",
    "    train_df.loc[val_index, 'fold'] = fold\n",
    "\n",
    "train_df['fold'] = train_df['fold'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['is_valid'] = train_df['fold'] == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    3129\n",
       "True      782\n",
       "Name: is_valid, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.is_valid.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2b: Define your `DataBlock`\n",
    "\n",
    "We'll use blurr's `set_seed()` method to ensure reproducibility (which is important as you iterate over different hyperparameters, explore different augmentation strategies, etc...). For a great discussion on how to do this in fastai, see the [\"[Solved] Reproducibility: Where is the randomness coming in?\"](https://forums.fast.ai/t/solved-reproducibility-where-is-the-randomness-coming-in/31628/25) post on the forums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(seed)\n",
    "sep = f\" {hf_tokenizer.sep_token} \"\n",
    "\n",
    "blocks = (TextBlock(hf_arch, hf_config, hf_tokenizer, hf_model, max_length=128), RegressionBlock(n_out = 6))\n",
    "dblock = DataBlock(\n",
    "    blocks=blocks,\n",
    "    get_x=ColReader('full_text'),\n",
    "    get_y=ColReader(target_cols),\n",
    "    splitter=ColSplitter(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2c: Get your `DataLoaders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(seed)\n",
    "dls = dblock.dataloaders(train_df, bs=batch_size) #.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Success consist of going from failure to failure withouth loss of enthusiasm, and I'm agree with this statement, because can do what it wants if try and try, and everything is doing with effort, and than a failure can be for a better oportunity in the future. Did you heard that failured means success sometime? well effort is the best way that you can reach what you wish. Can do what it wants if try and try, so let me tell you something about a friend that is close to me, and he was in failre to</td>\n",
       "      <td>(3.0, 2.5, 3.0, 2.5, 3.0, 2.5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Summer vacation is a good break for the students to spend time with their family, also a good way to take a break form school and your annoying classes, to take some rest, change your mind, and think about what will you change, when you go back to school, also taking a long summer vacation will get yourself out of stress. you will be able to do other think than just memorizing stuff you don't need in a real life, that's why it's not a good idea to reduce students summer break just because educat</td>\n",
       "      <td>(3.0, 3.5, 3.0, 3.5, 2.5, 3.0)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=2, trunc_at=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 4, torch.Size([4, 128]), 4)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = dls.one_batch()\n",
    "len(b), len(b[0][\"input_ids\"]), b[0][\"input_ids\"].shape, len(b[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[     1,  10159,   8200,    265,    446,    292,   2694,    264,   2694,\n",
       "             497,   1537,   1265,    265,   8143,    261,    263,    273,    280,\n",
       "             358,   1757,    275,    291,   1548,    261,    401,    295,    333,\n",
       "             339,    278,   1654,    337,    687,    263,    687,    261,    263,\n",
       "             758,    269,    653,    275,   1622,    366,    263,    354,    266,\n",
       "            2694,    295,    282,    270,    266,    493,   2673,   8690,  50451,\n",
       "             267,    262,    723,    260,   2709,    274,   1331,    272,   2694,\n",
       "             407,    666,   1228,   8684,    302,    371,   1622,    269,    262,\n",
       "             410,    384,    272,    274,    295,   1431,    339,    274,   1437,\n",
       "             260,   1194,    333,    339,    278,   1654,    337,    687,    263,\n",
       "             687,    261,    324,    678,    351,    848,    274,    491,    314,\n",
       "             266,   1156,    272,    269,    828,    264,    351,    261,    263,\n",
       "             313,    284,    267,   3703,    368,    264,   2694,    261,    324,\n",
       "             311,    326,    313,    670,    264,    351,    263,    313,    732,\n",
       "             351,      2],\n",
       "         [     1,    273,    700,    264,    836,    262,   2140,    265,    262,\n",
       "            2261,    483,    260,    273,    284,   1053,    272,    262,    483,\n",
       "             286,    370,   1117,    261,    263,    338,    334,    264,    374,\n",
       "             422,    260,    279,    465,    272,    584,    338,    334,    264,\n",
       "             374,    267,    291,    483,    269,    401,    261,    312,    774,\n",
       "             263,    312,   2649,    374,    267,   2261,    260,    879,    355,\n",
       "             504,    272,    269,    324,    657,    374,    422,    261,    304,\n",
       "             269,    311,   1719,    272,    584,    409,    264,    413,    260,\n",
       "             279,   2261,    483,    269,    266,    610,   4636,    358,  78129,\n",
       "             272,   1738,    355,    374,    263,    501,    355,    338,    334,\n",
       "             264,    374,    461,    260,    369,    584,    295,    286,    291,\n",
       "             688,    584,    338,    334,    264,    799,    267,    262,    537,\n",
       "             265,   1352,   9771,  45892,    260,    273,    409,    264,    374,\n",
       "             401,    261,    584,    286,   3968,    267,    312,    432,    263,\n",
       "             479,      2],\n",
       "         [     1, 112207,    843,    263,   1377,    269,    379,    282,    673,\n",
       "            2931,  12292,    270,    857,    432,    263,   1417,    260,   3407,\n",
       "            1581,    386,   1417,    479,    438,    570,    843,    263,   1377,\n",
       "             270,    308,   1323,    380,    260,    434,    738,  37642,    261,\n",
       "             380,    265,    529,    264,    365,   4027,    261,    319,    295,\n",
       "            4843,    262,    481,  18546,    265,  17918,   2232,    263,    529,\n",
       "             277,    262,   2574,    260,    879, 122064,    286,   4347,    262,\n",
       "             781,    265,    266,    654,    271,   1900,    374,    542,    324,\n",
       "             306,    295,  15324,    843,    263,   1377,    260,    336,    654,\n",
       "             271,   1900,    374,    542,    295,    327,   1290,    277,  44356,\n",
       "          122064,    263,    552,    349,    267,   1265,    304,    278,   3898,\n",
       "             277,    349,    361,    306,   2059,    308,    374,    267,    654,\n",
       "             538,    260,    420,   3432,   1657,   1111,    291,   1040,    293,\n",
       "            8867,    262,    563,    406,    375,    743,    263,    365,    266,\n",
       "             654,      2],\n",
       "         [     1,    273,    428,    272,    347,    479,    272,    301,    708,\n",
       "             298,    281,    980,    267,    262,    820,    262,    598,    306,\n",
       "             296,    374,    269,    298,    262,    410,   1622,    260,    450,\n",
       "             770,    272,    262,   1271,    265,    980,    269,    934,  23072,\n",
       "             269,    266,  38331,  17019,    260,    273,    413,    266,   3129,\n",
       "             277,    291,    889,    273,    428,    272,    296,    468,    267,\n",
       "             262,    723,    260,    273,    428,    272,    281,    467,    355,\n",
       "             268,    347,    604,    298,    468,    359,    723,    364,    428,\n",
       "             272,    262,    432,    349,   2435,    364,    265,    264,    685,\n",
       "             304,    298,    494,  11790,   1803,    272,    298,    269,    364,\n",
       "             432,    260,    273,    428,    272,    598,    286,    919,    265,\n",
       "           11086,    948,    474,    297,    272,    269,    291,    269,    299,\n",
       "            6238,    298,    269,    639,    301,    286,    266,   6238,   2489,\n",
       "             301,    286,    266,   6238,    267,    291,    432,    401,   2489,\n",
       "             298,      2]], device='cuda:0'),\n",
       " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0'),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0'),\n",
       " 'labels': tensor([[3.0000, 2.5000, 3.0000, 2.5000, 3.0000, 2.5000],\n",
       "         [2.5000, 2.5000, 3.0000, 2.5000, 2.0000, 2.5000],\n",
       "         [3.5000, 3.5000, 4.0000, 3.0000, 3.0000, 3.0000],\n",
       "         [2.5000, 2.5000, 3.5000, 2.5000, 2.5000, 2.5000]], device='cuda:0')}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[3.0000, 2.5000, 3.0000, 2.5000, 3.0000, 2.5000],\n",
       "         [2.5000, 2.5000, 3.0000, 2.5000, 2.0000, 2.5000],\n",
       "         [3.5000, 3.5000, 4.0000, 3.0000, 3.0000, 3.0000],\n",
       "         [2.5000, 2.5000, 3.5000, 2.5000, 2.5000, 2.5000]], device='cuda:0'),\n",
       " torch.Size([4, 6]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[1], b[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Train a model\n",
    "\n",
    "Sticking with blurr's mid-level API, let's define our `Learner`.  You can learn all about the blurr specific bits [here](https://ohmeow.github.io/blurr/text-modeling-core.html).\n",
    "\n",
    "Your metric should reflect your objective, and in the case of kaggle comps they tell you exactly what that is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def comp_metric_score(preds, targs):\n",
    "    \"\"\"This competition is evaluated using \"columnwise root mean squared error\". Expects numpy arrays.\"\"\"\n",
    "    len_target_cols = targs.shape[1]\n",
    "    score = [0] * len_target_cols\n",
    "    for i in range(len_target_cols):\n",
    "        score[i] = np.sqrt(mean_squared_error(preds[:, i], targs[:, i]))\n",
    "    return np.mean(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def MCRMSE(dim_argmax=None, **kwargs):\n",
    "    \"columnwise root mean squared error for regression problem\"\n",
    "    def mcrmse(x,y): return comp_metric_score(x,y)\n",
    "    return AccumMetric(mcrmse, invert_arg=False, flatten=False, dim_argmax=dim_argmax, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MCRMSE loss from here:\n",
    "\n",
    "https://www.kaggle.com/code/masashisode/pytorch-implementation-of-mcrmseloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSELoss_0(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, yhat, y):\n",
    "        loss = torch.sqrt(self.mse(yhat, y) + self.eps)\n",
    "        return loss\n",
    "\n",
    "\n",
    "class MCRMSELoss(nn.Module):\n",
    "    def __init__(self, num_scored=6):\n",
    "        super().__init__()\n",
    "        self.rmse = RMSELoss_0()\n",
    "        self.num_scored = num_scored\n",
    "\n",
    "    def forward(self, yhat, y):\n",
    "        score = 0\n",
    "        for i in range(self.num_scored):\n",
    "            score += self.rmse(yhat[:, i], y[:, i]) / self.num_scored\n",
    "\n",
    "        return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from fastai.losses import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(seed)\n",
    "\n",
    "model = BaseModelWrapper(hf_model)\n",
    "learn_cbs = [BaseModelCallback]\n",
    "fit_cbs = []\n",
    "\n",
    "learn = Learner(\n",
    "    dls,\n",
    "    model,\n",
    "    opt_func=partial(Adam, wd=weight_decay),\n",
    "    loss_func=MCRMSELoss(),\n",
    "    metrics=[MCRMSE()],\n",
    "    #loss_func=BCEWithLogitsLossFlat(),  # PreCalculatedBCELoss()\n",
    "    #metrics=[partial(accuracy_multi, thresh=0.2)],\n",
    "    #loss_func=MSELoss(),\n",
    "    #metrics=[rmse],\n",
    "    cbs=learn_cbs,\n",
    "    splitter=blurr_splitter,\n",
    ") #.to_fp16()\n",
    "\n",
    "#learn = learn.to_fp16()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "BaseModelWrapper (Input shape: 4 x 128)\n",
       "============================================================================\n",
       "Layer (type)         Output Shape         Param #    Trainable \n",
       "============================================================================\n",
       "                     4 x 128 x 768       \n",
       "Embedding                                 98380800   True      \n",
       "LayerNorm                                 1536       True      \n",
       "StableDropout                                                  \n",
       "Linear                                    590592     True      \n",
       "Linear                                    590592     True      \n",
       "Linear                                    590592     True      \n",
       "StableDropout                                                  \n",
       "StableDropout                                                  \n",
       "Linear                                    590592     True      \n",
       "LayerNorm                                 1536       True      \n",
       "StableDropout                                                  \n",
       "____________________________________________________________________________\n",
       "                     4 x 128 x 3072      \n",
       "Linear                                    2362368    True      \n",
       "GELUActivation                                                 \n",
       "____________________________________________________________________________\n",
       "                     4 x 128 x 768       \n",
       "Linear                                    2360064    True      \n",
       "LayerNorm                                 1536       True      \n",
       "StableDropout                                                  \n",
       "Linear                                    590592     True      \n",
       "Linear                                    590592     True      \n",
       "Linear                                    590592     True      \n",
       "StableDropout                                                  \n",
       "StableDropout                                                  \n",
       "Linear                                    590592     True      \n",
       "LayerNorm                                 1536       True      \n",
       "StableDropout                                                  \n",
       "____________________________________________________________________________\n",
       "                     4 x 128 x 3072      \n",
       "Linear                                    2362368    True      \n",
       "GELUActivation                                                 \n",
       "____________________________________________________________________________\n",
       "                     4 x 128 x 768       \n",
       "Linear                                    2360064    True      \n",
       "LayerNorm                                 1536       True      \n",
       "StableDropout                                                  \n",
       "Linear                                    590592     True      \n",
       "Linear                                    590592     True      \n",
       "Linear                                    590592     True      \n",
       "StableDropout                                                  \n",
       "StableDropout                                                  \n",
       "Linear                                    590592     True      \n",
       "LayerNorm                                 1536       True      \n",
       "StableDropout                                                  \n",
       "____________________________________________________________________________\n",
       "                     4 x 128 x 3072      \n",
       "Linear                                    2362368    True      \n",
       "GELUActivation                                                 \n",
       "____________________________________________________________________________\n",
       "                     4 x 128 x 768       \n",
       "Linear                                    2360064    True      \n",
       "LayerNorm                                 1536       True      \n",
       "StableDropout                                                  \n",
       "Linear                                    590592     True      \n",
       "Linear                                    590592     True      \n",
       "Linear                                    590592     True      \n",
       "StableDropout                                                  \n",
       "StableDropout                                                  \n",
       "Linear                                    590592     True      \n",
       "LayerNorm                                 1536       True      \n",
       "StableDropout                                                  \n",
       "____________________________________________________________________________\n",
       "                     4 x 128 x 3072      \n",
       "Linear                                    2362368    True      \n",
       "GELUActivation                                                 \n",
       "____________________________________________________________________________\n",
       "                     4 x 128 x 768       \n",
       "Linear                                    2360064    True      \n",
       "LayerNorm                                 1536       True      \n",
       "StableDropout                                                  \n",
       "Linear                                    590592     True      \n",
       "Linear                                    590592     True      \n",
       "Linear                                    590592     True      \n",
       "StableDropout                                                  \n",
       "StableDropout                                                  \n",
       "Linear                                    590592     True      \n",
       "LayerNorm                                 1536       True      \n",
       "StableDropout                                                  \n",
       "____________________________________________________________________________\n",
       "                     4 x 128 x 3072      \n",
       "Linear                                    2362368    True      \n",
       "GELUActivation                                                 \n",
       "____________________________________________________________________________\n",
       "                     4 x 128 x 768       \n",
       "Linear                                    2360064    True      \n",
       "LayerNorm                                 1536       True      \n",
       "StableDropout                                                  \n",
       "Linear                                    590592     True      \n",
       "Linear                                    590592     True      \n",
       "Linear                                    590592     True      \n",
       "StableDropout                                                  \n",
       "StableDropout                                                  \n",
       "Linear                                    590592     True      \n",
       "LayerNorm                                 1536       True      \n",
       "StableDropout                                                  \n",
       "____________________________________________________________________________\n",
       "                     4 x 128 x 3072      \n",
       "Linear                                    2362368    True      \n",
       "GELUActivation                                                 \n",
       "____________________________________________________________________________\n",
       "                     4 x 128 x 768       \n",
       "Linear                                    2360064    True      \n",
       "LayerNorm                                 1536       True      \n",
       "StableDropout                                                  \n",
       "LayerNorm                                 1536       True      \n",
       "Linear                                    590592     True      \n",
       "StableDropout                                                  \n",
       "____________________________________________________________________________\n",
       "                     4 x 6               \n",
       "Linear                                    4614       True      \n",
       "StableDropout                                                  \n",
       "____________________________________________________________________________\n",
       "\n",
       "Total params: 141,506,310\n",
       "Total trainable params: 141,506,310\n",
       "Total non-trainable params: 0\n",
       "\n",
       "Optimizer used: functools.partial(<function Adam at 0x7f05945e2550>, wd=0.01)\n",
       "Loss function: MCRMSELoss(\n",
       "  (rmse): RMSELoss_0(\n",
       "    (mse): MSELoss()\n",
       "  )\n",
       ")\n",
       "\n",
       "Callbacks:\n",
       "  - TrainEvalCallback\n",
       "  - BaseModelCallback\n",
       "  - CastToTensor\n",
       "  - Recorder\n",
       "  - ProgressCallback"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with torch.no_grad():\n",
    "#    foo = model.forward(b[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#foo, foo.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#b[1], b[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr_minimum, lr_steep, lr_valley, lr_slide = learn.lr_find(start_lr=1e-9, suggest_funcs=[minimum, steep, valley, slide])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0019952623173594476,\n",
       " 0.0003162277571391314,\n",
       " 0.0003162277571391314,\n",
       " 0.0001995262282434851)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_minimum, lr_steep, lr_valley, lr_slide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "slice(1e-05, 0.02, None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slice(*[1e-5, 2e-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mcrmse</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.590069</td>\n",
       "      <td>0.524998</td>\n",
       "      <td>0.560982</td>\n",
       "      <td>01:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.573292</td>\n",
       "      <td>0.730468</td>\n",
       "      <td>0.765047</td>\n",
       "      <td>02:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.544487</td>\n",
       "      <td>0.551028</td>\n",
       "      <td>0.586900</td>\n",
       "      <td>01:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.506508</td>\n",
       "      <td>0.541851</td>\n",
       "      <td>0.575684</td>\n",
       "      <td>01:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.495134</td>\n",
       "      <td>0.539350</td>\n",
       "      <td>0.575993</td>\n",
       "      <td>01:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.410843</td>\n",
       "      <td>0.512539</td>\n",
       "      <td>0.547041</td>\n",
       "      <td>01:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.404178</td>\n",
       "      <td>0.506759</td>\n",
       "      <td>0.540690</td>\n",
       "      <td>01:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.364305</td>\n",
       "      <td>0.493587</td>\n",
       "      <td>0.526696</td>\n",
       "      <td>01:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.340559</td>\n",
       "      <td>0.485799</td>\n",
       "      <td>0.518898</td>\n",
       "      <td>01:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.336492</td>\n",
       "      <td>0.485451</td>\n",
       "      <td>0.518319</td>\n",
       "      <td>01:52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(10, lr_max=slice(1e-5, 2e-2), cbs=fit_cbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# oof df\n",
    "preds, targs = learn.get_preds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([782, 6]), torch.Size([782, 6]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape, targs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pd.DataFrame(preds)\n",
    "preds.columns = [\"pred_\" + x for x in target_cols]\n",
    "targs = pd.DataFrame(targs)\n",
    "targs.columns = [\"targ_\" + x for x in target_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_cohesion</th>\n",
       "      <th>pred_syntax</th>\n",
       "      <th>pred_vocabulary</th>\n",
       "      <th>pred_phraseology</th>\n",
       "      <th>pred_grammar</th>\n",
       "      <th>pred_conventions</th>\n",
       "      <th>targ_cohesion</th>\n",
       "      <th>targ_syntax</th>\n",
       "      <th>targ_vocabulary</th>\n",
       "      <th>targ_phraseology</th>\n",
       "      <th>targ_grammar</th>\n",
       "      <th>targ_conventions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.733717</td>\n",
       "      <td>3.608072</td>\n",
       "      <td>3.765745</td>\n",
       "      <td>3.716778</td>\n",
       "      <td>3.703024</td>\n",
       "      <td>3.756756</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.252229</td>\n",
       "      <td>3.094465</td>\n",
       "      <td>3.317124</td>\n",
       "      <td>3.170558</td>\n",
       "      <td>3.113463</td>\n",
       "      <td>3.157710</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.826524</td>\n",
       "      <td>2.668943</td>\n",
       "      <td>2.924798</td>\n",
       "      <td>2.736864</td>\n",
       "      <td>2.649516</td>\n",
       "      <td>2.694057</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.614123</td>\n",
       "      <td>3.477585</td>\n",
       "      <td>3.652685</td>\n",
       "      <td>3.575231</td>\n",
       "      <td>3.547356</td>\n",
       "      <td>3.596668</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.645477</td>\n",
       "      <td>3.510763</td>\n",
       "      <td>3.679640</td>\n",
       "      <td>3.608292</td>\n",
       "      <td>3.575146</td>\n",
       "      <td>3.638493</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>777</th>\n",
       "      <td>2.239976</td>\n",
       "      <td>2.101528</td>\n",
       "      <td>2.398314</td>\n",
       "      <td>2.191833</td>\n",
       "      <td>2.089370</td>\n",
       "      <td>2.130233</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>2.383245</td>\n",
       "      <td>2.241170</td>\n",
       "      <td>2.523932</td>\n",
       "      <td>2.319600</td>\n",
       "      <td>2.219038</td>\n",
       "      <td>2.258724</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>3.725348</td>\n",
       "      <td>3.593027</td>\n",
       "      <td>3.754735</td>\n",
       "      <td>3.699827</td>\n",
       "      <td>3.675921</td>\n",
       "      <td>3.734006</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>3.285907</td>\n",
       "      <td>3.135614</td>\n",
       "      <td>3.349255</td>\n",
       "      <td>3.213486</td>\n",
       "      <td>3.155712</td>\n",
       "      <td>3.205565</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>3.957097</td>\n",
       "      <td>3.850498</td>\n",
       "      <td>3.969091</td>\n",
       "      <td>3.969115</td>\n",
       "      <td>3.958797</td>\n",
       "      <td>4.040996</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>782 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     pred_cohesion  pred_syntax  pred_vocabulary  pred_phraseology  \\\n",
       "0         3.733717     3.608072         3.765745          3.716778   \n",
       "1         3.252229     3.094465         3.317124          3.170558   \n",
       "2         2.826524     2.668943         2.924798          2.736864   \n",
       "3         3.614123     3.477585         3.652685          3.575231   \n",
       "4         3.645477     3.510763         3.679640          3.608292   \n",
       "..             ...          ...              ...               ...   \n",
       "777       2.239976     2.101528         2.398314          2.191833   \n",
       "778       2.383245     2.241170         2.523932          2.319600   \n",
       "779       3.725348     3.593027         3.754735          3.699827   \n",
       "780       3.285907     3.135614         3.349255          3.213486   \n",
       "781       3.957097     3.850498         3.969091          3.969115   \n",
       "\n",
       "     pred_grammar  pred_conventions  targ_cohesion  targ_syntax  \\\n",
       "0        3.703024          3.756756            3.5          4.0   \n",
       "1        3.113463          3.157710            3.0          3.0   \n",
       "2        2.649516          2.694057            3.5          2.5   \n",
       "3        3.547356          3.596668            3.5          3.0   \n",
       "4        3.575146          3.638493            4.0          3.5   \n",
       "..            ...               ...            ...          ...   \n",
       "777      2.089370          2.130233            3.5          2.5   \n",
       "778      2.219038          2.258724            2.5          2.0   \n",
       "779      3.675921          3.734006            2.5          3.0   \n",
       "780      3.155712          3.205565            4.0          4.0   \n",
       "781      3.958797          4.040996            4.0          4.5   \n",
       "\n",
       "     targ_vocabulary  targ_phraseology  targ_grammar  targ_conventions  \n",
       "0                3.5               3.5           4.0               4.0  \n",
       "1                3.5               3.5           3.0               3.0  \n",
       "2                3.5               3.0           3.0               3.0  \n",
       "3                3.5               3.5           3.5               4.0  \n",
       "4                3.0               4.0           3.5               4.0  \n",
       "..               ...               ...           ...               ...  \n",
       "777              3.0               3.0           2.0               2.5  \n",
       "778              2.5               1.5           2.0               2.0  \n",
       "779              3.0               4.0           3.5               3.0  \n",
       "780              4.0               4.0           3.5               3.0  \n",
       "781              4.5               4.0           4.5               4.5  \n",
       "\n",
       "[782 rows x 12 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oof_df = pd.concat([preds, targs], axis = 1)\n",
    "oof_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('kaggle_feedback_ell')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d2db59df3616dc6bdbad0dc2d7e2b7acca8412ddb42e2b6f3fed3a5dafe0c1bc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
