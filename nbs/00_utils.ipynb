{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |default_exp utils\n",
    "# |default_cls_lvl 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils\n",
    "\n",
    "> Competition utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "from __future__ import annotations\n",
    "\n",
    "import abc, datetime, random, os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from fastai.losses import CrossEntropyLossFlat\n",
    "from fastai.test_utils import show_install\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import pdb\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "# from fastcore.test import *\n",
    "from nbdev.showdoc import show_doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU #0: GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "# |hide\n",
    "# |cuda\n",
    "gpu_num = int(os.getenv(\"USE_GPU\", 0))\n",
    "\n",
    "torch.cuda.set_device(gpu_num)\n",
    "print(f\"Using GPU #{torch.cuda.current_device()}: {torch.cuda.get_device_name()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defaults\n",
    "\n",
    "Application wide defaults go here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "default_seed = int(os.getenv(\"RANDOM_SEED\", 42))\n",
    "kaggle_comp = os.getenv(\"KAGGLE_COMP\", \"feedback-prize-english-language-learning\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development environment\n",
    "\n",
    "Information about where your code is running and your compute capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def detect_env():\n",
    "    \"\"\"A helper function that detects where you are running code\"\"\"\n",
    "    if os.environ.get(\"KAGGLE_KERNEL_RUN_TYPE\", False):\n",
    "        run_env = \"kaggle\"\n",
    "    elif os.path.isdir(\"/content\"):\n",
    "        run_env = \"colab\"\n",
    "    elif os.path.isdir(\"../nbs\") or os.path.isdir(\"../../nbs\"):\n",
    "        run_env = \"local_nb\"\n",
    "    else:\n",
    "        run_env = \"script\"\n",
    "\n",
    "    return run_env\n",
    "\n",
    "\n",
    "run_env = detect_env()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local_nb\n"
     ]
    }
   ],
   "source": [
    "# |eval: false\n",
    "print(run_env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def print_dev_environment():\n",
    "    \"\"\"Provides details on your development environment including packages installed, cuda/cudnn availability, GPUs, etc.\"\"\"\n",
    "    print(show_install())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "```text\n",
      "=== Software === \n",
      "python        : 3.9.13\n",
      "fastai        : 2.7.9\n",
      "fastcore      : 1.5.26\n",
      "fastprogress  : 1.0.3\n",
      "torch         : 1.12.1+cu102\n",
      "nvidia driver : 460.106\n",
      "torch cuda    : 10.2 / is available\n",
      "torch cudnn   : 7605 / is enabled\n",
      "\n",
      "=== Hardware === \n",
      "nvidia gpus   : 3\n",
      "torch devices : 3\n",
      "  - gpu0      : GeForce GTX 1080 Ti\n",
      "  - gpu1      : GeForce GTX 1080 Ti\n",
      "  - gpu2      : GeForce GTX 1080\n",
      "\n",
      "=== Environment === \n",
      "platform      : Linux-5.4.0-124-generic-x86_64-with-glibc2.27\n",
      "distro        : #140~18.04.1-Ubuntu SMP Fri Aug 5 11:43:34 UTC 2022\n",
      "conda env     : kaggle_feedback_ell\n",
      "python        : /home/wgilliam/mambaforge/envs/kaggle_feedback_ell/bin/python\n",
      "sys.path      : /home/wgilliam/development/projects/kaggle/kaggle_feedback_ell/nbs\n",
      "/home/wgilliam/mambaforge/envs/kaggle_feedback_ell/lib/python39.zip\n",
      "/home/wgilliam/mambaforge/envs/kaggle_feedback_ell/lib/python3.9\n",
      "/home/wgilliam/mambaforge/envs/kaggle_feedback_ell/lib/python3.9/lib-dynload\n",
      "\n",
      "/home/wgilliam/.local/lib/python3.9/site-packages\n",
      "/home/wgilliam/mambaforge/envs/kaggle_feedback_ell/lib/python3.9/site-packages\n",
      "```\n",
      "\n",
      "Please make sure to include opening/closing ``` when you paste into forums/github to make the reports appear formatted as code sections.\n",
      "\n",
      "Optional package(s) to enhance the diagnostics can be installed with:\n",
      "pip install distro\n",
      "Once installed, re-run this utility to get the additional information\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# |output: false\n",
    "print_dev_environment()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Competition Setup\n",
    "\n",
    "**NOTE**: The first thing you should run when setting things up after you've deinfed your `kaggle_comp`, is `setup_comp()`. This method will ensure all the necessary folders are created as well as download the competition data if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def get_paths(override_project_root=None):\n",
    "    \"\"\"Returns data, models, and log folder paths based on your where you are running the code\"\"\"\n",
    "    if run_env == \"kaggle\":\n",
    "        data_path = Path(\".\")\n",
    "        comp_data_path = clean_data_path = Path(f\"../input/{kaggle_comp}\")\n",
    "        working_path = Path(\"/kaggle/working\")\n",
    "        models_path = working_path / \"models\"\n",
    "        logs_path = working_path / \"logs\"\n",
    "\n",
    "    elif run_env == \"colab\":\n",
    "        proj_root_path = override_project_root or Path(\".\")\n",
    "\n",
    "        data_path = proj_root_path\n",
    "        comp_data_path = clean_data_path = data_path\n",
    "        models_path = data_path / \"models\"\n",
    "        logs_path = data_path / \"logs\"\n",
    "\n",
    "    elif run_env == \"local_nb\":\n",
    "        proj_root_path = override_project_root or Path(\"..\")\n",
    "\n",
    "        data_path = Path(proj_root_path / \"data\")\n",
    "        comp_data_path = data_path / \"comp\"\n",
    "        clean_data_path = data_path / \"clean\"\n",
    "        models_path = Path(proj_root_path / \"models\")\n",
    "        logs_path = Path(proj_root_path / \"logs\")\n",
    "\n",
    "        comp_data_path.mkdir(parents=True, exist_ok=True)\n",
    "        clean_data_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    elif run_env == \"script\":\n",
    "        proj_root_path = override_project_root or Path(\".\")\n",
    "\n",
    "        data_path = Path(proj_root_path / \"data\")\n",
    "        comp_data_path = data_path / \"comp\"\n",
    "        clean_data_path = data_path / \"clean\"\n",
    "        models_path = Path(proj_root_path / \"models\")\n",
    "        logs_path = Path(proj_root_path / \"logs\")\n",
    "\n",
    "        comp_data_path.mkdir(parents=True, exist_ok=True)\n",
    "        clean_data_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        models_path.mkdir(parents=True, exist_ok=True)\n",
    "        logs_path.mkdir(parents=True, exist_ok=True)\n",
    "    except:\n",
    "        print(\"Unable to create models and logs folders\")\n",
    "\n",
    "    return data_path, comp_data_path, clean_data_path, models_path, logs_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def setup_comp(override_project_root=None, comp_data_path_override=None):\n",
    "    \"\"\"Ensures that the expected data, models, and logs folders exist and that the competition data exists in the 'comp_data_path'.\"\"\"\n",
    "\n",
    "    if comp_data_path_override is not None:\n",
    "        comp_data_path = comp_data_path_override\n",
    "    else:\n",
    "        _, comp_data_path, *_ = get_paths(override_project_root)\n",
    "\n",
    "    if run_env != \"kaggle\":\n",
    "        from kaggle import api\n",
    "        \n",
    "        if not comp_data_path.exists() or not any(comp_data_path.iterdir()):\n",
    "            import zipfile\n",
    "\n",
    "            api.competition_download_cli(kaggle_comp)\n",
    "\n",
    "            zipfile.ZipFile(f\"{kaggle_comp}.zip\").extractall(comp_data_path)\n",
    "            Path(f\"{kaggle_comp}.zip\").unlink(missing_ok=True)\n",
    "\n",
    "        return comp_data_path\n",
    "    else:\n",
    "        return Path(f\"../input/{kaggle_comp}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('../data/comp')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# |eval: false\n",
    "setup_comp()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Competition Metrics\n",
    "\n",
    "This competition is evaluated using \"multi-class logarithmic loss\" (e.g., cross-entropy loss). From the competition website ...\n",
    "\n",
    "> Each row in the dataset has been labeled with one true effectiveness label. For each row, you must submit the predicted probabilities that the product belongs to each quality label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def comp_metric_score(preds, targs):\n",
    "    \"\"\"This competition is evaluated using \"multi-class logarithmic loss\" (e.g., cross-entropy loss). Expects numpy arrays.\"\"\"\n",
    "    probs = np.exp(preds) / np.sum(np.exp(preds), axis=1, keepdims=True)\n",
    "\n",
    "    correct_class_probs = probs[range(len(preds)), targs]\n",
    "    nll = -np.log(correct_class_probs)\n",
    "    return nll.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify our comp_metric_score calculates cross-entropy loss correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.1897,  0.4049,  0.4013,  0.3084,  2.3549],\n",
      "        [ 0.8280, -0.0217, -0.4138,  0.3544,  1.8640],\n",
      "        [-0.4228, -0.0550,  0.7587, -1.7377, -0.1868]])\n",
      "tensor([0, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "preds = torch.randn(3, 5)\n",
    "targets = torch.tensor([0, 3, 4])\n",
    "\n",
    "print(preds)\n",
    "print(targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.6128), TensorBase(1.6128), 1.6128138)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_loss = F.cross_entropy(preds, targets)\n",
    "fastai_loss = CrossEntropyLossFlat()(preds, targets)\n",
    "np_loss = comp_metric_score(preds.numpy(), targets.numpy())\n",
    "\n",
    "# test_close(pytorch_loss.item(), fastai_loss.item())\n",
    "# test_close(pytorch_loss.item(), np_loss)\n",
    "\n",
    "pytorch_loss, fastai_loss, np_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def get_run_id():\n",
    "    run_id = str(datetime.datetime.now())[:16].replace(\":\", \"_\").replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "    return run_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022_09_13_23_10\n"
     ]
    }
   ],
   "source": [
    "print(get_run_id())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('kaggle_feedback_ell')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
